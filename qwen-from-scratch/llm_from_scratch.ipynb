{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yvesemmanuel/deep-learning/blob/main/qwen-from-scratch/llm_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ne0Mrk73T5dz",
        "outputId": "13629b21-4c5e-47cb-976d-52ffe4fe1130"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-11-19 21:57:47,574 - INFO - Logger is working!\n"
          ]
        }
      ],
      "source": [
        "import logging\n",
        "import sys\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "logger.handlers.clear()\n",
        "logger.propagate = False\n",
        "\n",
        "handler = logging.StreamHandler(sys.stdout)\n",
        "handler.setLevel(logging.INFO)\n",
        "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "handler.setFormatter(formatter)\n",
        "\n",
        "logger.addHandler(handler)\n",
        "\n",
        "logger.info(\"Logger is working!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "X4XfLoK5hoVO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gWOs2lBO9S_"
      },
      "source": [
        "# Modern Language Model Architecture\n",
        "\n",
        "The building blocks for the Transformer architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "SKsl4aD9eDvN"
      },
      "outputs": [],
      "source": [
        "SEED = 1332\n",
        "CHOOSE_MODEL = \"14B\"\n",
        "USE_BASE_MODEL = False\n",
        "USE_REASONING_MODEL = True\n",
        "USE_INSTRUCT_MODEL = False\n",
        "\n",
        "if (USE_BASE_MODEL + USE_REASONING_MODEL\n",
        "    + USE_INSTRUCT_MODEL) != 1:\n",
        "    raise AttributeError(\"Only one of the options above can be True.\")\n",
        "\n",
        "if CHOOSE_MODEL == \"0.6B\":\n",
        "    QWEN3_CONFIG = {\n",
        "        \"vocab_size\": 151_936,           # Vocabulary size\n",
        "        \"context_length\": 40_960,        # Context length that was used to train the model\n",
        "        \"emb_dim\": 1024,                 # Embedding dimension\n",
        "        \"n_heads\": 16,                   # Number of attention heads\n",
        "        \"n_layers\": 28,                  # Number of layers\n",
        "        \"hidden_dim\": 3072,              # Size of the intermediate dimension in FeedForward\n",
        "        \"head_dim\": 128,                 # Size of the heads in GQA\n",
        "        \"qk_norm\": True,                 # Whether to normalize queries and keys in GQA\n",
        "        \"n_kv_groups\": 8,                # Key-Value groups for grouped-query attention\n",
        "        \"rope_base\": 1_000_000.0,        # The base in RoPE's \"theta\"\n",
        "        \"dtype\": torch.bfloat16,         # Lower-precision dtype to reduce memory usage\n",
        "    }\n",
        "\n",
        "elif CHOOSE_MODEL == \"1.7B\":\n",
        "    QWEN3_CONFIG = {\n",
        "        \"vocab_size\": 151_936,\n",
        "        \"context_length\": 40_960,\n",
        "        \"emb_dim\": 2048,                 # 2x larger than above\n",
        "        \"n_heads\": 16,\n",
        "        \"n_layers\": 28,\n",
        "        \"hidden_dim\": 6144,              # 2x larger than above\n",
        "        \"head_dim\": 128,\n",
        "        \"qk_norm\": True,\n",
        "        \"n_kv_groups\": 8,\n",
        "        \"rope_base\": 1_000_000.0,\n",
        "        \"dtype\": torch.bfloat16,\n",
        "    }\n",
        "\n",
        "elif CHOOSE_MODEL == \"4B\":\n",
        "    QWEN3_CONFIG = {\n",
        "        \"vocab_size\": 151_936,\n",
        "        \"context_length\": 40_960,\n",
        "        \"emb_dim\": 2560,                 # 25% larger than above\n",
        "        \"n_heads\": 32,                   # 2x larger than above\n",
        "        \"n_layers\": 36,                  # 29% larger than above\n",
        "        \"hidden_dim\": 9728,              # ~3x larger than above\n",
        "        \"head_dim\": 128,\n",
        "        \"qk_norm\": True,\n",
        "        \"n_kv_groups\": 8,\n",
        "        \"rope_base\": 1_000_000.0,\n",
        "        \"dtype\": torch.bfloat16,\n",
        "    }\n",
        "\n",
        "elif CHOOSE_MODEL == \"8B\":\n",
        "    QWEN3_CONFIG = {\n",
        "        \"vocab_size\": 151_936,\n",
        "        \"context_length\": 40_960,\n",
        "        \"emb_dim\": 4096,                 # 60% larger than above\n",
        "        \"n_heads\": 32,\n",
        "        \"n_layers\": 36,                  # 26% larger than above\n",
        "        \"hidden_dim\": 12288,\n",
        "        \"head_dim\": 128,\n",
        "        \"qk_norm\": True,\n",
        "        \"n_kv_groups\": 8,\n",
        "        \"rope_base\": 1_000_000.0,\n",
        "        \"dtype\": torch.bfloat16,\n",
        "    }\n",
        "\n",
        "elif CHOOSE_MODEL == \"14B\":\n",
        "    QWEN3_CONFIG = {\n",
        "        \"vocab_size\": 151_936,\n",
        "        \"context_length\": 40_960,\n",
        "        \"emb_dim\": 5120,                 # 25% larger than above\n",
        "        \"n_heads\": 40,                   # 25% larger than above\n",
        "        \"n_layers\": 40,                  # 11% larger than above\n",
        "        \"hidden_dim\": 17408,             # 42% larger than above\n",
        "        \"head_dim\": 128,\n",
        "        \"qk_norm\": True,\n",
        "        \"n_kv_groups\": 8,\n",
        "        \"rope_base\": 1_000_000.0,\n",
        "        \"dtype\": torch.bfloat16,\n",
        "    }\n",
        "\n",
        "elif CHOOSE_MODEL == \"32B\":\n",
        "    QWEN3_CONFIG = {\n",
        "        \"vocab_size\": 151_936,\n",
        "        \"context_length\": 40_960,\n",
        "        \"emb_dim\": 5120,\n",
        "        \"n_heads\": 64,                   # 60% larger than above\n",
        "        \"n_layers\": 64,                  # 60% larger than above\n",
        "        \"hidden_dim\": 25600,             # 47% larger than above\n",
        "        \"head_dim\": 128,\n",
        "        \"qk_norm\": True,\n",
        "        \"n_kv_groups\": 8,\n",
        "        \"rope_base\": 1_000_000.0,\n",
        "        \"dtype\": torch.bfloat16,\n",
        "    }\n",
        "\n",
        "else:\n",
        "    raise ValueError(f\"{CHOOSE_MODEL} is not supported.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GATmPYzQPJgK"
      },
      "source": [
        "### The **Feed Forward**\n",
        "\n",
        "These are the layers that actually store information about the world."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NfRNdUkmNdI_"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
        "        self.fc2 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
        "        self.fc3 = nn.Linear(cfg[\"hidden_dim\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_fc1 = self.fc1(x)\n",
        "        x_fc2 = self.fc2(x)\n",
        "        x = nn.functional.silu(x_fc1) * x_fc2\n",
        "        return self.fc3(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0-CVgsvPnDo"
      },
      "source": [
        "### **Root Mean Square Layer Normalization**\n",
        "\n",
        "Layers for training stabilization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "y54hea2LOmwS"
      },
      "outputs": [],
      "source": [
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, emb_dim, eps=1e-6, bias=False, qwen3_compatible=True):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.qwen3_compatible = qwen3_compatible\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim)) if bias else None\n",
        "\n",
        "    def forward(self, x):\n",
        "        input_dtype = x.dtype\n",
        "\n",
        "        if self.qwen3_compatible:\n",
        "            x = x.to(torch.float32)\n",
        "\n",
        "        variance = x.pow(2).mean(dim=-1, keepdim=True)\n",
        "        norm_x = x * torch.rsqrt(variance + self.eps)\n",
        "        norm_x = norm_x * self.scale\n",
        "\n",
        "        if self.shift is not None:\n",
        "            norm_x = norm_x + self.shift\n",
        "\n",
        "        return norm_x.to(input_dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8ddqKf8Qdd5"
      },
      "source": [
        "### **Grouped Query Attention Mechanism**\n",
        "\n",
        "Helps the model to focus the most importance parts of the input sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Q7SIxpgvOojA"
      },
      "outputs": [],
      "source": [
        "def compute_rope_params(head_dim, theta_base=10_000, context_length=4096, dtype=torch.float32):\n",
        "    assert head_dim % 2 == 0, \"Embedding dimension must be even\"\n",
        "\n",
        "    # Compute the inverse frequencies\n",
        "    inv_freq = 1.0 / (theta_base ** (torch.arange(0, head_dim, 2, dtype=dtype)[: (head_dim // 2)].float() / head_dim))\n",
        "\n",
        "    # Generate position indices\n",
        "    positions = torch.arange(context_length, dtype=dtype)\n",
        "\n",
        "    # Compute the angles\n",
        "    angles = positions.unsqueeze(1) * inv_freq.unsqueeze(0)  # Shape: (context_length, head_dim // 2)\n",
        "\n",
        "    # Expand angles to match the head_dim\n",
        "    angles = torch.cat([angles, angles], dim=1)  # Shape: (context_length, head_dim)\n",
        "\n",
        "    # Precompute sine and cosine\n",
        "    cos = torch.cos(angles)\n",
        "    sin = torch.sin(angles)\n",
        "\n",
        "    return cos, sin\n",
        "\n",
        "\n",
        "def apply_rope(x, cos, sin, offset=0):\n",
        "    # x: (batch_size, num_heads, seq_len, head_dim)\n",
        "    batch_size, num_heads, seq_len, head_dim = x.shape\n",
        "    assert head_dim % 2 == 0, \"Head dimension must be even\"\n",
        "\n",
        "    # Split x into first half and second half\n",
        "    x1 = x[..., : head_dim // 2]  # First half\n",
        "    x2 = x[..., head_dim // 2:]  # Second half\n",
        "\n",
        "    # Adjust sin and cos shapes\n",
        "    cos = cos[offset:offset + seq_len, :].unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, seq_len, head_dim)\n",
        "    sin = sin[offset:offset + seq_len, :].unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "    # Apply the rotary transformation\n",
        "    rotated = torch.cat((-x2, x1), dim=-1)\n",
        "    x_rotated = (x * cos) + (rotated * sin)\n",
        "\n",
        "    # It's ok to use lower-precision after applying cos and sin rotation\n",
        "    return x_rotated.to(dtype=x.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "iBla0_wzOuJm"
      },
      "outputs": [],
      "source": [
        "class GroupedQueryAttention(nn.Module):\n",
        "    def __init__(\n",
        "        self, d_in, num_heads, num_kv_groups, head_dim=None, qk_norm=False, dtype=None\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert num_heads % num_kv_groups == 0, \"num_heads must be divisible by num_kv_groups\"\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.num_kv_groups = num_kv_groups\n",
        "        self.group_size = num_heads // num_kv_groups\n",
        "\n",
        "        if head_dim is None:\n",
        "            assert d_in % num_heads == 0, \"`d_in` must be divisible by `num_heads` if `head_dim` is not set\"\n",
        "            head_dim = d_in // num_heads\n",
        "\n",
        "        self.head_dim = head_dim\n",
        "        self.d_out = num_heads * head_dim\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, self.d_out, bias=False, dtype=dtype)\n",
        "        self.W_key = nn.Linear(d_in, num_kv_groups * head_dim, bias=False, dtype=dtype)\n",
        "        self.W_value = nn.Linear(d_in, num_kv_groups * head_dim, bias=False, dtype=dtype)\n",
        "\n",
        "        self.out_proj = nn.Linear(self.d_out, d_in, bias=False, dtype=dtype)\n",
        "\n",
        "        if qk_norm:\n",
        "            self.q_norm = RMSNorm(head_dim, eps=1e-6)\n",
        "            self.k_norm = RMSNorm(head_dim, eps=1e-6)\n",
        "        else:\n",
        "            self.q_norm = self.k_norm = None\n",
        "\n",
        "    def forward(self, x, mask, cos, sin, start_pos=0, cache=None):\n",
        "        b, num_tokens, _ = x.shape\n",
        "\n",
        "        # Apply projections\n",
        "        queries = self.W_query(x)  # (b, num_tokens, num_heads * head_dim)\n",
        "        keys = self.W_key(x)       # (b, num_tokens, num_kv_groups * head_dim)\n",
        "        values = self.W_value(x)   # (b, num_tokens, num_kv_groups * head_dim)\n",
        "\n",
        "        # Reshape\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        keys_new = keys.view(b, num_tokens, self.num_kv_groups, self.head_dim).transpose(1, 2)\n",
        "        values_new = values.view(b, num_tokens, self.num_kv_groups, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Optional normalization\n",
        "        if self.q_norm:\n",
        "            queries = self.q_norm(queries)\n",
        "        if self.k_norm:\n",
        "            keys_new = self.k_norm(keys_new)\n",
        "\n",
        "        # Apply RoPE\n",
        "        queries = apply_rope(queries, cos, sin, offset=start_pos)\n",
        "        keys_new = apply_rope(keys_new, cos, sin, offset=start_pos)\n",
        "\n",
        "        if cache is not None:\n",
        "            prev_k, prev_v = cache\n",
        "            keys = torch.cat([prev_k, keys_new], dim=2)\n",
        "            values = torch.cat([prev_v, values_new], dim=2)\n",
        "            next_cache = (keys, values)\n",
        "        else:\n",
        "            start_pos = 0  # reset RoPE\n",
        "            keys, values = keys_new, values_new\n",
        "            next_cache = (keys, values)\n",
        "\n",
        "        # Expand K and V to match number of heads\n",
        "        keys = keys.repeat_interleave(self.group_size, dim=1)\n",
        "        values = values.repeat_interleave(self.group_size, dim=1)\n",
        "\n",
        "        # Attention\n",
        "        attn_scores = queries @ keys.transpose(2, 3)\n",
        "        attn_scores = attn_scores.masked_fill(mask, -torch.inf)\n",
        "        attn_weights = torch.softmax(attn_scores / self.head_dim**0.5, dim=-1)\n",
        "\n",
        "        context = (attn_weights @ values).transpose(1, 2).reshape(b, num_tokens, self.d_out)\n",
        "        return self.out_proj(context), next_cache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWdYe2L6SARz"
      },
      "source": [
        "### The heart of modern sequence architectures; the **Transformers** block!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "JEFecuyiO4q2"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.att = GroupedQueryAttention(\n",
        "            d_in=cfg[\"emb_dim\"],\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            head_dim=cfg[\"head_dim\"],\n",
        "            num_kv_groups=cfg[\"n_kv_groups\"],\n",
        "            qk_norm=cfg[\"qk_norm\"],\n",
        "            dtype=cfg[\"dtype\"]\n",
        "        )\n",
        "        self.ff = FeedForward(cfg)\n",
        "        self.norm1 = RMSNorm(cfg[\"emb_dim\"], eps=1e-6)\n",
        "        self.norm2 = RMSNorm(cfg[\"emb_dim\"], eps=1e-6)\n",
        "\n",
        "    def forward(self, x, mask, cos, sin, start_pos=0, cache=None):\n",
        "        # Shortcut connection for attention block\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x, next_cache = self.att(x, mask, cos, sin, start_pos=start_pos, cache=cache)  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        # Shortcut connection for feed-forward block\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ff(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        return x, next_cache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zh4QLOZZSG2o"
      },
      "source": [
        "### **Qwen3 Architecture**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1s5dwivFO6wl"
      },
      "outputs": [],
      "source": [
        "class Qwen3Model(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "\n",
        "        # Main model parameters\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"])\n",
        "\n",
        "        self.trf_blocks = nn.ModuleList(  # ModuleList since Sequential can only accept one input, and we need `x, mask, cos, sin`\n",
        "            [TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
        "        )\n",
        "        self.final_norm = RMSNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False, dtype=cfg[\"dtype\"])\n",
        "\n",
        "        # Reusable utilities\n",
        "        if cfg[\"head_dim\"] is None:\n",
        "            head_dim = cfg[\"emb_dim\"] // cfg[\"n_heads\"]\n",
        "        else:\n",
        "            head_dim = cfg[\"head_dim\"]\n",
        "        cos, sin = compute_rope_params(\n",
        "            head_dim=head_dim,\n",
        "            theta_base=cfg[\"rope_base\"],\n",
        "            context_length=cfg[\"context_length\"]\n",
        "        )\n",
        "        self.register_buffer(\"cos\", cos, persistent=False)\n",
        "        self.register_buffer(\"sin\", sin, persistent=False)\n",
        "        self.cfg = cfg\n",
        "        self.current_pos = 0  # Track current position in KV cache\n",
        "\n",
        "    def forward(self, in_idx, cache=None):\n",
        "        # Forward pass\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        x = tok_embeds\n",
        "\n",
        "        num_tokens = x.shape[1]\n",
        "        if cache is not None:\n",
        "            pos_start = self.current_pos\n",
        "            pos_end = pos_start + num_tokens\n",
        "            self.current_pos = pos_end\n",
        "            mask = torch.triu(\n",
        "                torch.ones(pos_end, pos_end, device=x.device, dtype=torch.bool), diagonal=1\n",
        "            )[pos_start:pos_end, :pos_end]\n",
        "        else:\n",
        "            pos_start = 0  # Not strictly necessary but helps torch.compile\n",
        "            mask = torch.triu(\n",
        "                torch.ones(num_tokens, num_tokens, device=x.device, dtype=torch.bool), diagonal=1\n",
        "            )\n",
        "        # Shape (1, 1, num_tokens, num_tokens) to broadcast across batch and heads\n",
        "        mask = mask[None, None, :, :]\n",
        "\n",
        "        for i, block in enumerate(self.trf_blocks):\n",
        "            blk_cache = cache.get(i) if cache else None\n",
        "            x, new_blk_cache = block(x, mask, self.cos, self.sin,\n",
        "                                     start_pos=pos_start,\n",
        "                                     cache=blk_cache)\n",
        "            if cache is not None:\n",
        "                cache.update(i, new_blk_cache)\n",
        "\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x.to(self.cfg[\"dtype\"]))\n",
        "        return logits\n",
        "\n",
        "    def reset_kv_cache(self):\n",
        "        self.current_pos = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hl1l8Zr6ewed"
      },
      "source": [
        "### **Key-Value Cache**\n",
        "\n",
        "KV Caching helps speed up the next token generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0xBy9_NAeuCI"
      },
      "outputs": [],
      "source": [
        "class KVCache:\n",
        "    def __init__(self, n_layers):\n",
        "        self.cache = [None] * n_layers\n",
        "\n",
        "    def get(self, layer_idx):\n",
        "        return self.cache[layer_idx]\n",
        "\n",
        "    def update(self, layer_idx, value):\n",
        "        self.cache[layer_idx] = value\n",
        "\n",
        "    def get_all(self):\n",
        "        return self.cache\n",
        "\n",
        "    def reset(self):\n",
        "        for i in range(len(self.cache)):\n",
        "            self.cache[i] = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cViE3esjSugT"
      },
      "source": [
        "# Model instantiation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiCF-0dmTAUd",
        "outputId": "a5430322-e0fa-4db6-ffcd-675d518111e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-11-19 21:57:50,980 - INFO - cuda\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "logger.info(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lw-VhYsKfG_I",
        "outputId": "59a1e6a9-d334-4d07-bc4a-e24a645f8978"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f1227fcdad0>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.manual_seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNFvsAR-TDa3",
        "outputId": "1f19908a-685d-44e9-eb48-ace16d387cf5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Qwen3Model(\n",
              "  (tok_emb): Embedding(151936, 5120)\n",
              "  (trf_blocks): ModuleList(\n",
              "    (0-39): 40 x TransformerBlock(\n",
              "      (att): GroupedQueryAttention(\n",
              "        (W_query): Linear(in_features=5120, out_features=5120, bias=False)\n",
              "        (W_key): Linear(in_features=5120, out_features=1024, bias=False)\n",
              "        (W_value): Linear(in_features=5120, out_features=1024, bias=False)\n",
              "        (out_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
              "        (q_norm): RMSNorm()\n",
              "        (k_norm): RMSNorm()\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (fc1): Linear(in_features=5120, out_features=17408, bias=False)\n",
              "        (fc2): Linear(in_features=5120, out_features=17408, bias=False)\n",
              "        (fc3): Linear(in_features=17408, out_features=5120, bias=False)\n",
              "      )\n",
              "      (norm1): RMSNorm()\n",
              "      (norm2): RMSNorm()\n",
              "    )\n",
              "  )\n",
              "  (final_norm): RMSNorm()\n",
              "  (out_head): Linear(in_features=5120, out_features=151936, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = Qwen3Model(QWEN3_CONFIG)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuzVdhvjTFO0",
        "outputId": "b55d8c13-8602-4a0f-8087-8fe93987a487"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[-0.5352, -0.3438,  1.2031,  ..., -0.4316,  0.2129, -0.1001],\n",
              "         [-1.1719,  0.2559,  0.4629,  ..., -0.6875,  0.1475, -0.0493],\n",
              "         [-0.7031,  0.2393,  0.5391,  ..., -0.8828, -0.1211,  0.3750]]],\n",
              "       dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model(torch.tensor([1, 2, 3]).unsqueeze(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RK6sK3A8TMFb",
        "outputId": "87bdbbc4-fd76-4279-9180-6a2654b948c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-11-19 22:00:15,418 - INFO - Total number of parameters: 14,768,307,200\n",
            "2025-11-19 22:00:15,419 - INFO - \n",
            "Total number of unique parameters: 13,990,394,880\n"
          ]
        }
      ],
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "logger.info(f\"Total number of parameters: {total_params:,}\")\n",
        "\n",
        "# Account for weight tying\n",
        "total_params_normalized = total_params - model.tok_emb.weight.numel()\n",
        "logger.info(f\"\\nTotal number of unique parameters: {total_params_normalized:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqzYmRoSULy2",
        "outputId": "a0f52e23-9ae6-481e-cc6b-7d002a48683d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-11-19 22:00:15,428 - INFO - float32 (PyTorch default): 110.07 GB\n",
            "2025-11-19 22:00:15,430 - INFO - bfloat16: 55.04 GB\n"
          ]
        }
      ],
      "source": [
        "def model_memory_size(model, input_dtype=torch.float32):\n",
        "    total_params = 0\n",
        "    total_grads = 0\n",
        "    for param in model.parameters():\n",
        "        # Calculate total number of elements per parameter\n",
        "        param_size = param.numel()\n",
        "        total_params += param_size\n",
        "        # Check if gradients are stored for this parameter\n",
        "        if param.requires_grad:\n",
        "            total_grads += param_size\n",
        "\n",
        "    # Calculate buffer size (non-parameters that require memory)\n",
        "    total_buffers = sum(buf.numel() for buf in model.buffers())\n",
        "\n",
        "    # Size in bytes = (Number of elements) * (Size of each element in bytes)\n",
        "    # We assume parameters and gradients are stored in the same type as input dtype\n",
        "    element_size = torch.tensor(0, dtype=input_dtype).element_size()\n",
        "    total_memory_bytes = (total_params + total_grads + total_buffers) * element_size\n",
        "\n",
        "    # Convert bytes to gigabytes\n",
        "    total_memory_gb = total_memory_bytes / (1024**3)\n",
        "\n",
        "    return total_memory_gb\n",
        "\n",
        "logger.info(f\"float32 (PyTorch default): {model_memory_size(model, input_dtype=torch.float32):.2f} GB\")\n",
        "logger.info(f\"bfloat16: {model_memory_size(model, input_dtype=torch.bfloat16):.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "3SmIb0MtfR2F"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "model.to(device);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wa-2m2E4UY73"
      },
      "source": [
        "# Load pre-trained weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Gx-7AGZMUbHL"
      },
      "outputs": [],
      "source": [
        "def load_weights_into_qwen(model, param_config, params):\n",
        "    def assign(left, right, tensor_name=\"unknown\"):\n",
        "        if left.shape != right.shape:\n",
        "            raise ValueError(f\"Shape mismatch in tensor '{tensor_name}'. Left: {left.shape}, Right: {right.shape}\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if isinstance(right, torch.Tensor):\n",
        "                left.copy_(right)\n",
        "            else:\n",
        "                left.copy_(torch.as_tensor(right, dtype=left.dtype, device=left.device))\n",
        "\n",
        "        return left\n",
        "\n",
        "    model.tok_emb.weight = assign(model.tok_emb.weight, params[\"model.embed_tokens.weight\"], \"model.embed_tokens.weight\")\n",
        "\n",
        "    for l in range(param_config[\"n_layers\"]):\n",
        "        block = model.trf_blocks[l]\n",
        "        att = block.att\n",
        "\n",
        "        # Q, K, V projections\n",
        "        att.W_query.weight = assign(\n",
        "            att.W_query.weight,\n",
        "            params[f\"model.layers.{l}.self_attn.q_proj.weight\"],\n",
        "            f\"model.layers.{l}.self_attn.q_proj.weight\"\n",
        "        )\n",
        "        att.W_key.weight = assign(\n",
        "            att.W_key.weight,\n",
        "            params[f\"model.layers.{l}.self_attn.k_proj.weight\"],\n",
        "            f\"model.layers.{l}.self_attn.k_proj.weight\"\n",
        "        )\n",
        "        att.W_value.weight = assign(\n",
        "            att.W_value.weight,\n",
        "            params[f\"model.layers.{l}.self_attn.v_proj.weight\"],\n",
        "            f\"model.layers.{l}.self_attn.v_proj.weight\"\n",
        "        )\n",
        "\n",
        "        # Output projection\n",
        "        att.out_proj.weight = assign(\n",
        "            att.out_proj.weight,\n",
        "            params[f\"model.layers.{l}.self_attn.o_proj.weight\"],\n",
        "            f\"model.layers.{l}.self_attn.o_proj.weight\"\n",
        "        )\n",
        "\n",
        "        # QK norms\n",
        "        if hasattr(att, \"q_norm\") and att.q_norm is not None:\n",
        "            att.q_norm.scale = assign(\n",
        "                att.q_norm.scale,\n",
        "                params[f\"model.layers.{l}.self_attn.q_norm.weight\"],\n",
        "                f\"model.layers.{l}.self_attn.q_norm.weight\"\n",
        "            )\n",
        "        if hasattr(att, \"k_norm\") and att.k_norm is not None:\n",
        "            att.k_norm.scale = assign(\n",
        "                att.k_norm.scale,\n",
        "                params[f\"model.layers.{l}.self_attn.k_norm.weight\"],\n",
        "                f\"model.layers.{l}.self_attn.k_norm.weight\"\n",
        "            )\n",
        "\n",
        "        # Attention layernorm\n",
        "        block.norm1.scale = assign(\n",
        "            block.norm1.scale,\n",
        "            params[f\"model.layers.{l}.input_layernorm.weight\"],\n",
        "            f\"model.layers.{l}.input_layernorm.weight\"\n",
        "        )\n",
        "\n",
        "        # Feedforward weights\n",
        "        block.ff.fc1.weight = assign(\n",
        "            block.ff.fc1.weight,\n",
        "            params[f\"model.layers.{l}.mlp.gate_proj.weight\"],\n",
        "            f\"model.layers.{l}.mlp.gate_proj.weight\"\n",
        "        )\n",
        "        block.ff.fc2.weight = assign(\n",
        "            block.ff.fc2.weight,\n",
        "            params[f\"model.layers.{l}.mlp.up_proj.weight\"],\n",
        "            f\"model.layers.{l}.mlp.up_proj.weight\"\n",
        "        )\n",
        "        block.ff.fc3.weight = assign(\n",
        "            block.ff.fc3.weight,\n",
        "            params[f\"model.layers.{l}.mlp.down_proj.weight\"],\n",
        "            f\"model.layers.{l}.mlp.down_proj.weight\"\n",
        "        )\n",
        "        block.norm2.scale = assign(\n",
        "            block.norm2.scale,\n",
        "            params[f\"model.layers.{l}.post_attention_layernorm.weight\"],\n",
        "            f\"model.layers.{l}.post_attention_layernorm.weight\"\n",
        "        )\n",
        "\n",
        "    # Final normalization and output head\n",
        "    model.final_norm.scale = assign(model.final_norm.scale, params[\"model.norm.weight\"], \"model.norm.weight\")\n",
        "\n",
        "    if \"lm_head.weight\" in params:\n",
        "        model.out_head.weight = assign(model.out_head.weight, params[\"lm_head.weight\"], \"lm_head.weight\")\n",
        "    else:\n",
        "        model.out_head.weight = model.tok_emb.weight\n",
        "        logger.info(\"Model uses weight tying.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "37b117865ee64c4d97c6439ef5544f2d",
            "f4fc6328d67245b88eb01ae69fbf0086",
            "799cd7e0a61a4b15b919535041bfe3ac",
            "c64a320414fa4324b40fb4f02d8ecde0",
            "7d756fb0bfb749bd8b456c3d2ddbd99e",
            "30f5f0b2cc8843abaa8e5016754c9789",
            "1bcbb74a430549cda326d64c280175fe",
            "12825e3c83f24e6b8023a29b82c68e32",
            "97e83a76e8404c1ea1e694afd7fbc3c6",
            "4a07f77153b9439193dd036ad4f48c6c",
            "2189608e7d7a4385a276bfbc923fe281"
          ]
        },
        "id": "HitnzXr2UeHP",
        "outputId": "a3182fa7-2faf-47d4-e380-079900f81862"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "37b117865ee64c4d97c6439ef5544f2d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 18 files:   0%|          | 0/18 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "from safetensors.torch import load_file\n",
        "from huggingface_hub import hf_hub_download, snapshot_download\n",
        "\n",
        "\n",
        "if USE_REASONING_MODEL or USE_INSTRUCT_MODEL:\n",
        "    repo_id = f\"Qwen/Qwen3-{CHOOSE_MODEL}\"\n",
        "else:\n",
        "    repo_id = f\"Qwen/Qwen3-{CHOOSE_MODEL}-Base\"\n",
        "\n",
        "local_dir = Path(repo_id).parts[-1]\n",
        "\n",
        "if CHOOSE_MODEL == \"0.6B\":\n",
        "    weights_file = hf_hub_download(\n",
        "        repo_id=repo_id,\n",
        "        filename=\"model.safetensors\",\n",
        "        local_dir=local_dir,\n",
        "    )\n",
        "    weights_dict = load_file(weights_file)\n",
        "else:\n",
        "    repo_dir = snapshot_download(repo_id=repo_id, local_dir=local_dir)\n",
        "    index_path = os.path.join(repo_dir, \"model.safetensors.index.json\")\n",
        "    with open(index_path, \"r\") as f:\n",
        "        index = json.load(f)\n",
        "\n",
        "    weights_dict = {}\n",
        "    for filename in set(index[\"weight_map\"].values()):\n",
        "        shard_path = os.path.join(repo_dir, filename)\n",
        "        shard = load_file(shard_path)\n",
        "        weights_dict.update(shard)\n",
        "\n",
        "load_weights_into_qwen(model, QWEN3_CONFIG, weights_dict)\n",
        "model.to(device)\n",
        "del weights_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKSSmWRLUlun"
      },
      "source": [
        "# Load tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "YzBW-xBJUnnD"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from tokenizers import Tokenizer\n",
        "\n",
        "class Qwen3Tokenizer:\n",
        "    _SPECIALS = [\n",
        "        \"<|endoftext|>\",\n",
        "        \"<|im_start|>\", \"<|im_end|>\",\n",
        "        \"<|object_ref_start|>\", \"<|object_ref_end|>\",\n",
        "        \"<|box_start|>\", \"<|box_end|>\",\n",
        "        \"<|quad_start|>\", \"<|quad_end|>\",\n",
        "        \"<|vision_start|>\", \"<|vision_end|>\",\n",
        "        \"<|vision_pad|>\", \"<|image_pad|>\", \"<|video_pad|>\",\n",
        "        \"<think>\", \"</think>\"\n",
        "    ]\n",
        "    _SPLIT_RE = re.compile(r\"(<\\|[^>]+?\\|>|<think>|</think>)\")\n",
        "\n",
        "    def __init__(self, tokenizer_file_path=\"tokenizer.json\", repo_id=None,\n",
        "                 apply_chat_template=True, add_generation_prompt=False, add_thinking=False):\n",
        "\n",
        "        self.apply_chat_template = apply_chat_template\n",
        "        self.add_generation_prompt = add_generation_prompt\n",
        "        self.add_thinking = add_thinking\n",
        "\n",
        "        tok_file = Path(tokenizer_file_path)\n",
        "        self._tok = Tokenizer.from_file(str(tok_file))\n",
        "        self._special_to_id = {}\n",
        "        for t in self._SPECIALS:\n",
        "            tid = self._tok.token_to_id(t)\n",
        "            if tid is not None:\n",
        "                self._special_to_id[t] = tid\n",
        "\n",
        "        self.pad_token_id = self._special_to_id[\"<|endoftext|>\"]\n",
        "        self.eos_token_id = self.pad_token_id\n",
        "\n",
        "        if repo_id and \"Base\" not in repo_id:\n",
        "            eos_token = \"<|im_end|>\"\n",
        "        else:\n",
        "            eos_token = \"<|endoftext|>\"\n",
        "        if eos_token in self._special_to_id:\n",
        "            self.eos_token_id = self._special_to_id[eos_token]\n",
        "\n",
        "    def encode(self, text, chat_wrapped=None):\n",
        "        if chat_wrapped is None:\n",
        "            chat_wrapped = self.apply_chat_template\n",
        "\n",
        "        stripped = text.strip()\n",
        "        if stripped in self._special_to_id and \"\\n\" not in stripped:\n",
        "            return [self._special_to_id[stripped]]\n",
        "\n",
        "        if chat_wrapped:\n",
        "            text = self._wrap_chat(text)\n",
        "\n",
        "        ids = []\n",
        "        for part in filter(None, self._SPLIT_RE.split(text)):\n",
        "            if part in self._special_to_id:\n",
        "                ids.append(self._special_to_id[part])\n",
        "            else:\n",
        "                ids.extend(self._tok.encode(part).ids)\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        return self._tok.decode(ids, skip_special_tokens=False)\n",
        "\n",
        "    def _wrap_chat(self, user_msg):\n",
        "        s = f\"<|im_start|>user\\n{user_msg}<|im_end|>\\n\"\n",
        "        if self.add_generation_prompt:\n",
        "            s += \"<|im_start|>assistant\"\n",
        "            if self.add_thinking:\n",
        "                s += \"\\n\"\n",
        "            else:\n",
        "                s += \"\\n<think>\\n\\n</think>\\n\\n\"\n",
        "        return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "OPvGP2vnUtPd"
      },
      "outputs": [],
      "source": [
        "if USE_REASONING_MODEL:\n",
        "    tokenizer_file_path = f\"Qwen3-{CHOOSE_MODEL}/tokenizer.json\"\n",
        "else:\n",
        "    tokenizer_file_path = f\"Qwen3-{CHOOSE_MODEL}-Base/tokenizer.json\"\n",
        "\n",
        "hf_hub_download(\n",
        "    repo_id=repo_id,\n",
        "    filename=\"tokenizer.json\",\n",
        "    local_dir=local_dir,\n",
        ")\n",
        "\n",
        "if USE_REASONING_MODEL or USE_INSTRUCT_MODEL:\n",
        "    tokenizer = Qwen3Tokenizer(\n",
        "        tokenizer_file_path=tokenizer_file_path,\n",
        "        repo_id=repo_id,\n",
        "        apply_chat_template=True,\n",
        "        add_generation_prompt=True,\n",
        "        add_thinking=USE_REASONING_MODEL\n",
        "    )\n",
        "\n",
        "else:\n",
        "    tokenizer = Qwen3Tokenizer(\n",
        "        tokenizer_file_path=tokenizer_file_path,\n",
        "        repo_id=repo_id,\n",
        "        apply_chat_template=False,\n",
        "        add_generation_prompt=False,\n",
        "        add_thinking=False\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "tx1LUt-uUvbn",
        "outputId": "cd970bb3-4115-415c-fed3-1773ed73561b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<|im_start|>user\\nimplement a python code for quicksort<|im_end|>\\n<|im_start|>assistant\\n'"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = \"implement a python code for quicksort\"\n",
        "\n",
        "input_token_ids = tokenizer.encode(prompt)\n",
        "text = tokenizer.decode(input_token_ids)\n",
        "text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlQfZY8xU3Bb"
      },
      "source": [
        "# Generate text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "JSUW9qRoU3sZ"
      },
      "outputs": [],
      "source": [
        "def generate_text_basic_stream(model, token_ids, max_new_tokens, eos_token_id=None, context_size=None):\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        cache = KVCache(n_layers=model.cfg[\"n_layers\"])\n",
        "        model.reset_kv_cache()\n",
        "\n",
        "        # Prime the cache with the initial context\n",
        "        logits = model(token_ids, cache=cache)\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            next_token = torch.argmax(logits[:, -1], dim=-1, keepdim=True)\n",
        "\n",
        "            if eos_token_id is not None and torch.all(next_token == eos_token_id):\n",
        "                break\n",
        "\n",
        "            yield next_token\n",
        "\n",
        "            token_ids = torch.cat([token_ids, next_token], dim=1)\n",
        "\n",
        "            # Feed only the new token to the model; cache handles history\n",
        "            logits = model(next_token, cache=cache)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NI4rAaCGU8Cv",
        "outputId": "b0a7cd1c-29dd-40ad-fc69-ec35bcbc0caf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<think>\n",
            "Okay, I need to implement the quicksort algorithm in Python. Let me think about how quicksort works. From what I remember, quicksort is a divide-and-conquer algorithm. It works by selecting a 'pivot' element from the array and partitioning the other elements into two sub-arrays: those less than the pivot and those greater than the pivot. Then it recursively sorts the sub-arrays.\n",
            "\n",
            "First, I should decide how to choose the pivot. Common strategies are picking the first element, the last, the middle, or a random element. For simplicity, maybe I'll start with the last element as the pivot. That way, I can use the standard partitioning approach.\n",
            "\n",
            "So the steps would be:\n",
            "\n",
            "1. Choose the pivot (let's say the last element).\n",
            "2. Partition the array into elements less than the pivot, equal to the pivot, and greater than the pivot.\n",
            "3. Recursively apply quicksort to the left and right partitions.\n",
            "\n",
            "Wait, but in some implementations, they just partition into elements less than or equal and greater than, and then combine. Maybe I need to write a helper function to partition the array.\n",
            "\n",
            "Let me outline the steps for the partition function. Suppose I have a list, and I need to rearrange it so that elements less than the pivot come before, and elements greater come after. The pivot is then in its correct position.\n",
            "\n",
            "Let me think of the partition function. Let's take the last element as pivot. Then, I'll have two pointers: one starting at the beginning (i) and another moving through the array (j). For each element, if it's less than the pivot, I swap it with the element at index i and increment i. At the end, I swap the pivot with the element at i, so that the pivot is in the correct place.\n",
            "\n",
            "Wait, here's a standard approach:\n",
            "\n",
            "Function quicksort(arr, low, high):\n",
            "    if low < high:\n",
            "        pi = partition(arr, low, high)\n",
            "        quicksort(arr, low, pi - 1)\n",
            "        quicksort(arr, pi + 1, high)\n",
            "\n",
            "Partition function:\n",
            "def partition(arr, low, high):\n",
            "    pivot = arr[high]\n",
            "    i = low - 1\n",
            "    for j in range(low, high):\n",
            "        if arr[j] <= pivot:\n",
            "            i += 1\n",
            "            arr[i], arr[j] = arr[j], arr[i]\n",
            "    arr[i+1], arr[high] = arr[high], arr[i+1]\n",
            "    return i + 1\n",
            "\n",
            "Yes, that's the standard Lomuto partition scheme. But I need to make sure that the indices are handled correctly.\n",
            "\n",
            "So, in Python, the list is passed by reference, so the function will modify the original list.\n",
            "\n",
            "But how to handle the initial call? The user would call quicksort with the entire list, so the initial call would be quicksort(arr, 0, len(arr)-1).\n",
            "\n",
            "But maybe I should write a wrapper function that takes the list and then calls the recursive function with the initial parameters.\n",
            "\n",
            "Alternatively, I can have a function that takes the list and then performs the sorting in place.\n",
            "\n",
            "Let me try to code this.\n",
            "\n",
            "First, the partition function. Let me write a helper function.\n",
            "\n",
            "Wait, but in Python, functions can be nested. But maybe it's easier to have the partition function inside the quicksort function, or as a separate function.\n",
            "\n",
            "Alternatively, here's a possible code structure:\n",
            "\n",
            "def quicksort(arr):\n",
            "    if len(arr) <= 1:\n",
            "        return arr\n",
            "    pivot = arr[-1]\n",
            "    left = [x for x in arr[:-1] if x < pivot]\n",
            "    middle = [x for x in arr if x == pivot]\n",
            "    right = [x for x in arr[:-1] if x > pivot]\n",
            "    return quicksort(left) + middle + quicksort(right)\n",
            "\n",
            "Wait, that's a more functional approach, creating new lists each time. But this is not in-place. However, it's easier to write. But for large arrays, this may use more memory. However, for the purpose of an example, this might be acceptable.\n",
            "\n",
            "But the original question says \"implement a python code for quicksort\". It doesn't specify whether it should be in-place or not. Both approaches are valid. However, the in-place version is more efficient in terms of space.\n",
            "\n",
            "But the functional approach is easier to code. Let me think. Which one is better to present?\n",
            "\n",
            "Well, the user might be looking for the standard in-place implementation. Let me try that.\n",
            "\n",
            "So, the in-place version. Let me write the code.\n",
            "\n",
            "def quicksort(arr, low, high):\n",
            "    if low < high:\n",
            "        pi = partition(arr, low, high)\n",
            "        quicksort(arr, low, pi - 1)\n",
            "        quicksort(arr, pi + 1, high)\n",
            "\n",
            "def partition(arr, low, high):\n",
            "    pivot = arr[high]\n",
            "    i = low - 1\n",
            "    for j in range(low, high):\n",
            "        if arr[j] <= pivot:\n",
            "            i += 1\n",
            "            arr[i], arr[j] = arr[j], arr[i]\n",
            "    arr[i+1], arr[high] = arr[high], arr[i+1]\n",
            "    return i + 1\n",
            "\n",
            "But then, the user would need to call this with the initial parameters. So, perhaps a helper function:\n",
            "\n",
            "def quicksort_wrapper(arr):\n",
            "    quicksort(arr, 0, len(arr)-1)\n",
            "\n",
            "But then, the user would have to pass the array, and the function would sort it in place.\n",
            "\n",
            "But in Python, lists are mutable, so the original list would be modified.\n",
            "\n",
            "So, for example:\n",
            "\n",
            "arr = [5, 3, 8, 4, 2]\n",
            "quicksort_wrapper(arr)\n",
            "print(arr)  # Output: [2, 3, 4, 5, 8]\n",
            "\n",
            "But the code above would work. However, the code for the partition function may have an error. Let me test with an example.\n",
            "\n",
            "Take the array [5,3,8,4,2]. Let's say we call partition with low=0, high=4 (since the array is 0-based). The pivot is 2. Then, i starts at -1. Then j runs from 0 to 3 (since high is 4, and the loop is range(low, high), which is 0 to 3 inclusive.\n",
            "\n",
            "For j=0, arr[j] is 5. 5 <= 2? No. So nothing.\n",
            "\n",
            "j=1: arr[j] is 3. 3 <= 2? No. Nothing.\n",
            "\n",
            "j=2: 8 <= 2? No.\n",
            "\n",
            "j=3: 4 <= 2? No. So the loop ends. Then, i is -1. Then, swap arr[i+1] (arr[0]) with arr[high] (arr[4]). So the array becomes [2,3,8,4,5]. The pivot is now at index 0. Then, the next steps would sort the left (from 0 to -1, which is not called) and the right from 1 to 4. But that seems correct. Wait, but in this case, the pivot is the smallest element, so the left partition is empty, and the right partition is the rest. Then, the next call would be to sort the right part.\n",
            "\n",
            "But in this case, the code would work. However, the problem is that the pivot is the last element, and in this case, the code would work. But perhaps the code is correct.\n",
            "\n",
            "But let me think of another example. Let's take [3, 1, 2]. Let's say we call partition with low=0, high=2. pivot is 2. i starts at -1. j runs from 0 to 1.\n",
            "\n",
            "j=0: arr[j] is 3. 3 <= 2? No. So nothing.\n",
            "\n",
            "j=1: arr[j] is 1. 1 <= 2: yes. i becomes 0. swap arr[0] and arr[1]. Now the array is [1,3,2]. Then, after the loop, swap arr[i+1] (arr[1]) with arr[high] (arr[2]). So swap 3 and 2. The array becomes [1,2,3]. The pivot is at index 1. So the partition returns 1. Then, the quicksort would call on left (0 to 0) and right (2 to 2). Which would sort correctly.\n",
            "\n",
            "So that seems to work.\n",
            "\n",
            "Another example: [5, 4, 3, 2, 1]. Let's see. The first pivot is 1. The partition would move all elements less than 1 (none) and then swap the pivot to the first position. Then the right partition is [5,4,3,2], which would be sorted similarly. But this would result in O(n^2) time, which is worst case for quicksort. But that's the nature of the algorithm when the array is already sorted. However, the code is correct, but the choice of pivot (last element) leads to worst case.\n",
            "\n",
            "But the code is correct regardless.\n",
            "\n",
            "So, the code for the in-place version is as above.\n",
            "\n",
            "But the user may want a function that takes a list and returns a sorted list, rather than modifying in place. So perhaps the functional approach is better for that.\n",
            "\n",
            "But the functional approach may be less efficient, but for the purpose of code, it's easier to write.\n",
            "\n",
            "So, the code for the functional approach:\n",
            "\n",
            "def quicksort(arr):\n",
            "    if len(arr) <= 1:\n",
            "        return arr\n",
            "    pivot = arr[-1]\n",
            "    left = [x for x in arr[:-1] if x < pivot]\n",
            "    middle = [x for x in arr if x == pivot]\n",
            "    right = [x for x in arr[:-1] if x > pivot]\n",
            "    return quicksort(left) + middle + quicksort(right)\n",
            "\n",
            "But this is not in-place. For example, if you call:\n",
            "\n",
            "arr = [5,3,8,4,2]\n",
            "sorted_arr = quicksort(arr)\n",
            "print(sorted_arr)\n",
            "\n",
            "But the original arr is not modified. However, this code is easier to read and write.\n",
            "\n",
            "But which one is better to present? The user didn't specify. But since the question is to implement quicksort, both are valid. However, the in-place version is more efficient.\n",
            "\n",
            "But perhaps the user expects the in-place version. However, the code for the in-place version requires the helper functions and the initial call.\n",
            "\n",
            "So, perhaps the code can be written as:\n",
            "\n",
            "def quicksort(arr):\n",
            "    def _quicksort(arr, low, high):\n",
            "        if low < high:\n",
            "            pi = partition(arr, low, high)\n",
            "            _quicksort(arr, low, pi-1)\n",
            "            _quicksort(arr, pi+1, high)\n",
            "    \n",
            "    def partition(arr, low, high):\n",
            "        pivot = arr[high]\n",
            "        i = low - 1\n",
            "        for j in range(low, high):\n",
            "            if arr[j] <= pivot:\n",
            "                i += 1\n",
            "                arr[i], arr[j] = arr[j], arr[i]\n",
            "        arr[i+1], arr[high] = arr[high], arr[i+1]\n",
            "        return i + 1\n",
            "    \n",
            "    _quicksort(arr, 0, len(arr)-1)\n",
            "\n",
            "But then, the function quicksort would modify the input array in place. So, for example:\n",
            "\n",
            "arr = [5,3,8,4,2]\n",
            "quicksort(arr)\n",
            "print(arr)  # [2,3,4,5,8]\n",
            "\n",
            "This is the in-place version. However, the code uses nested functions. But in Python, that's acceptable.\n",
            "\n",
            "Alternatively, the user may want a version that returns a new list. So, perhaps providing both options.\n",
            "\n",
            "But the original question says \"implement a python code for quicksort\". So, perhaps the user wants the standard in-place version. However, the code for that requires the helper functions.\n",
            "\n",
            "But in the code above, the quicksort function is written to modify the input array. However, in Python, lists are mutable, so this is possible.\n",
            "\n",
            "But perhaps the user would prefer a version that returns a new list. Let me think. For example, the functional approach is more Pythonic in some ways, but the in-place is more efficient.\n",
            "\n",
            "But the code for the in-place version may have some issues. For example, in the partition function, when the array is of length 1, but the code is called with low=0 and high=0. Then, the if low < high is false, so nothing is done. Which is correct.\n",
            "\n",
            "Another thing to note is that the pivot selection is the last element. So, the code is correct, but the choice of pivot can affect performance.\n",
            "\n",
            "But the code is correct.\n",
            "\n",
            "So, putting it all together, the code for the in-place version would be:\n",
            "\n",
            "def quicksort(arr):\n",
            "    def _quicksort(arr, low, high):\n",
            "        if low < high:\n",
            "            pi = partition(arr, low, high)\n",
            "            _quicksort(arr, low, pi - 1)\n",
            "            _quicksort(arr, pi + 1, high)\n",
            "    \n",
            "    def partition(arr, low, high):\n",
            "        pivot = arr[high]\n",
            "        i = low - 1\n",
            "        for j in range(low, high):\n",
            "            if arr[j] <= pivot:\n",
            "                i += 1\n",
            "                arr[i], arr[j] = arr[j], arr[i]\n",
            "        arr[i + 1], arr[high] = arr[high], arr[i + 1]\n",
            "        return i + 1\n",
            "    \n",
            "    _quicksort(arr, 0, len(arr) - 1)\n",
            "\n",
            "But then, the user can call this function with a list, and it will be sorted in place.\n",
            "\n",
            "Alternatively, if the user wants a function that returns a new sorted list, then the functional approach is better.\n",
            "\n",
            "But the user may not be aware of the difference. So, perhaps providing both options.\n",
            "\n",
            "But the original question says \"implement a python code for quicksort\". So, perhaps the user wants the standard implementation. Let me check some references.\n",
            "\n",
            "In standard implementations, quicksort is often done in-place. So, the code I wrote above is correct.\n",
            "\n",
            "But I need to make sure that the code is correct. Let me test it with"
          ]
        }
      ],
      "source": [
        "input_token_ids_tensor = torch.tensor(input_token_ids, device=device).unsqueeze(0)\n",
        "\n",
        "for token in generate_text_basic_stream(\n",
        "    model=model,\n",
        "    token_ids=input_token_ids_tensor,\n",
        "    max_new_tokens=3000,\n",
        "    eos_token_id=tokenizer.eos_token_id\n",
        "):\n",
        "    token_id = token.squeeze(0).tolist()\n",
        "    print(\n",
        "        tokenizer.decode(token_id),\n",
        "        end=\"\",\n",
        "        flush=True\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyMB8k397bdiWd6OcgO3kSaJ",
      "collapsed_sections": [
        "GATmPYzQPJgK",
        "J0-CVgsvPnDo",
        "b8ddqKf8Qdd5",
        "BWdYe2L6SARz",
        "Zh4QLOZZSG2o",
        "hl1l8Zr6ewed",
        "cViE3esjSugT"
      ],
      "gpuType": "A100",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "12825e3c83f24e6b8023a29b82c68e32": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1bcbb74a430549cda326d64c280175fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2189608e7d7a4385a276bfbc923fe281": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "30f5f0b2cc8843abaa8e5016754c9789": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37b117865ee64c4d97c6439ef5544f2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f4fc6328d67245b88eb01ae69fbf0086",
              "IPY_MODEL_799cd7e0a61a4b15b919535041bfe3ac",
              "IPY_MODEL_c64a320414fa4324b40fb4f02d8ecde0"
            ],
            "layout": "IPY_MODEL_7d756fb0bfb749bd8b456c3d2ddbd99e"
          }
        },
        "4a07f77153b9439193dd036ad4f48c6c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "799cd7e0a61a4b15b919535041bfe3ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12825e3c83f24e6b8023a29b82c68e32",
            "max": 18,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_97e83a76e8404c1ea1e694afd7fbc3c6",
            "value": 18
          }
        },
        "7d756fb0bfb749bd8b456c3d2ddbd99e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97e83a76e8404c1ea1e694afd7fbc3c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c64a320414fa4324b40fb4f02d8ecde0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a07f77153b9439193dd036ad4f48c6c",
            "placeholder": "",
            "style": "IPY_MODEL_2189608e7d7a4385a276bfbc923fe281",
            "value": "18/18[00:00&lt;00:00,839.56it/s]"
          }
        },
        "f4fc6328d67245b88eb01ae69fbf0086": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30f5f0b2cc8843abaa8e5016754c9789",
            "placeholder": "",
            "style": "IPY_MODEL_1bcbb74a430549cda326d64c280175fe",
            "value": "Fetching18files:100%"
          }
        }
      },
      "state": {
        "_dom_classes": [],
        "_model_module": "@jupyter-widgets/controls",
        "_model_module_version": "1.5.0",
        "_model_name": "HBoxModel",
        "_view_count": null,
        "_view_module": "@jupyter-widgets/controls",
        "_view_module_version": "1.5.0",
        "_view_name": "HBoxView",
        "box_style": "",
        "children": [
          "IPY_MODEL_f4fc6328d67245b88eb01ae69fbf0086",
          "IPY_MODEL_799cd7e0a61a4b15b919535041bfe3ac",
          "IPY_MODEL_c64a320414fa4324b40fb4f02d8ecde0"
        ],
        "layout": "IPY_MODEL_7d756fb0bfb749bd8b456c3d2ddbd99e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
