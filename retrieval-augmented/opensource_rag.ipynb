{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kA2aaclfNlgr",
    "outputId": "7eced416-91d6-4eb4-9d78-cde8bf56653d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-17 22:24:35,064 - INFO - Logger is working!\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "logger.handlers.clear()\n",
    "logger.propagate = False\n",
    "\n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "handler.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "\n",
    "logger.addHandler(handler)\n",
    "\n",
    "logger.info(\"Logger is working!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E8V89pE6BFXk",
    "outputId": "249f7840-1fe1-4e02-f6a2-499745b48a52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.48.0\n",
      "  Downloading transformers-4.48.0-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[?25l     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/44.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.0) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.0) (0.35.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.0) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.0) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.0) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.0) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.0) (2.32.4)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers==4.48.0)\n",
      "  Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.0) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.0) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.0) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.0) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.0) (1.1.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.48.0) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.48.0) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.48.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.48.0) (2025.10.5)\n",
      "Downloading transformers-4.48.0-py3-none-any.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m122.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.22.1\n",
      "    Uninstalling tokenizers-0.22.1:\n",
      "      Successfully uninstalled tokenizers-0.22.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.57.1\n",
      "    Uninstalling transformers-4.57.1:\n",
      "      Successfully uninstalled transformers-4.57.1\n",
      "Successfully installed tokenizers-0.21.4 transformers-4.48.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.48.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hna2Ktag6MZH",
    "outputId": "fa899045-2c21-45c9-9dfd-fa69a5288af3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting chromadb\n",
      "  Downloading chromadb-1.1.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\n",
      "Collecting pymupdf\n",
      "  Downloading pymupdf-1.26.5-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.3.0)\n",
      "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.11.10)\n",
      "Collecting pybase64>=1.4.1 (from chromadb)\n",
      "  Downloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.37.0)\n",
      "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.0.2)\n",
      "Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n",
      "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.15.0)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
      "  Downloading onnxruntime-1.23.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.38.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.21.4)\n",
      "Collecting pypika>=0.48.9 (from chromadb)\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
      "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.75.1)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb)\n",
      "  Downloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.19.2)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb)\n",
      "  Downloading kubernetes-34.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (8.5.0)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.0.3)\n",
      "Collecting mmh3>=4.0.1 (from chromadb)\n",
      "  Downloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.11.3)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.28.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.25.1)\n",
      "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (25.0)\n",
      "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (4.11.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.27.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.4)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Collecting urllib3<2.4.0,>=1.24.2 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.9.23)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.3)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.38.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.38.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.38.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_proto-1.38.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_sdk-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_api-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.59b0 (from opentelemetry-sdk>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.4.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.13.2->chromadb) (0.35.3)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (8.3.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
      "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.1.10)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Downloading chromadb-1.1.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.9 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m19.9/19.9 MB\u001b[0m \u001b[31m123.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pymupdf-1.26.5-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl (278 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kubernetes-34.1.0-py2.py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m110.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (103 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading onnxruntime-1.23.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.38.0-py3-none-any.whl (19 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.38.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_proto-1.38.0-py3-none-any.whl (72 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_sdk-1.38.0-py3-none-any.whl (132 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_api-1.38.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m65.9/65.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl (207 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
      "Downloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (517 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (4.4 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m113.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (456 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pypika\n",
      "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=6d9485af47414191ed736b635e3ef33bdce55abc2e110da286a147dbdd72d51f\n",
      "  Stored in directory: /root/.cache/pip/wheels/d5/3d/69/8d68d249cd3de2584f226e27fd431d6344f7d70fd856ebd01b\n",
      "Successfully built pypika\n",
      "Installing collected packages: pypika, durationpy, uvloop, urllib3, pymupdf, pybase64, opentelemetry-proto, mmh3, humanfriendly, httptools, bcrypt, backoff, watchfiles, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, posthog, opentelemetry-semantic-conventions, onnxruntime, opentelemetry-sdk, kubernetes, opentelemetry-exporter-otlp-proto-grpc, chromadb\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.5.0\n",
      "    Uninstalling urllib3-2.5.0:\n",
      "      Successfully uninstalled urllib3-2.5.0\n",
      "  Attempting uninstall: opentelemetry-proto\n",
      "    Found existing installation: opentelemetry-proto 1.37.0\n",
      "    Uninstalling opentelemetry-proto-1.37.0:\n",
      "      Successfully uninstalled opentelemetry-proto-1.37.0\n",
      "  Attempting uninstall: opentelemetry-exporter-otlp-proto-common\n",
      "    Found existing installation: opentelemetry-exporter-otlp-proto-common 1.37.0\n",
      "    Uninstalling opentelemetry-exporter-otlp-proto-common-1.37.0:\n",
      "      Successfully uninstalled opentelemetry-exporter-otlp-proto-common-1.37.0\n",
      "  Attempting uninstall: opentelemetry-api\n",
      "    Found existing installation: opentelemetry-api 1.37.0\n",
      "    Uninstalling opentelemetry-api-1.37.0:\n",
      "      Successfully uninstalled opentelemetry-api-1.37.0\n",
      "  Attempting uninstall: opentelemetry-semantic-conventions\n",
      "    Found existing installation: opentelemetry-semantic-conventions 0.58b0\n",
      "    Uninstalling opentelemetry-semantic-conventions-0.58b0:\n",
      "      Successfully uninstalled opentelemetry-semantic-conventions-0.58b0\n",
      "  Attempting uninstall: opentelemetry-sdk\n",
      "    Found existing installation: opentelemetry-sdk 1.37.0\n",
      "    Uninstalling opentelemetry-sdk-1.37.0:\n",
      "      Successfully uninstalled opentelemetry-sdk-1.37.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-adk 1.16.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\n",
      "google-adk 1.16.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n",
      "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.38.0 which is incompatible.\n",
      "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.38.0 which is incompatible.\n",
      "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed backoff-2.2.1 bcrypt-5.0.0 chromadb-1.1.1 coloredlogs-15.0.1 durationpy-0.10 httptools-0.7.1 humanfriendly-10.0 kubernetes-34.1.0 mmh3-5.2.0 onnxruntime-1.23.1 opentelemetry-api-1.38.0 opentelemetry-exporter-otlp-proto-common-1.38.0 opentelemetry-exporter-otlp-proto-grpc-1.38.0 opentelemetry-proto-1.38.0 opentelemetry-sdk-1.38.0 opentelemetry-semantic-conventions-0.59b0 posthog-5.4.0 pybase64-1.4.2 pymupdf-1.26.5 pypika-0.48.9 urllib3-2.3.0 uvloop-0.22.1 watchfiles-1.1.1\n",
      "Collecting flash-attn\n",
      "  Downloading flash_attn-2.8.3.tar.gz (8.4 MB)\n",
      "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from flash-attn) (2.8.0+cu126)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from flash-attn) (0.8.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->flash-attn) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->flash-attn) (3.0.3)\n",
      "Building wheels for collected packages: flash-attn\n",
      "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for flash-attn: filename=flash_attn-2.8.3-cp312-cp312-linux_x86_64.whl size=256040057 sha256=f25da18657a87fc83dc1bfb8b7751b82246e9db355510226b674fd437c34b5fb\n",
      "  Stored in directory: /root/.cache/pip/wheels/3d/59/46/f282c12c73dd4bb3c2e3fe199f1a0d0f8cec06df0cccfeee27\n",
      "Successfully built flash-attn\n",
      "Installing collected packages: flash-attn\n",
      "Successfully installed flash-attn-2.8.3\n"
     ]
    }
   ],
   "source": [
    "!pip install chromadb pymupdf\n",
    "!pip install flash-attn --no-build-isolatio # Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NoFCJNjTVJtw"
   },
   "source": [
    "### **Fetch PDF as bytes**\n",
    "\n",
    "Download and save the PDF locally to verify proper formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xS5Nf56sVH8B"
   },
   "outputs": [],
   "source": [
    "pdf_url = \"https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CoXGGkSSVKkz"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O05omvp0VLJr"
   },
   "outputs": [],
   "source": [
    "def fetch_pdf_from_url(url: str, save_path: Optional[str] = None) -> bytes:\n",
    "  \"\"\"\n",
    "  Fetch a PDF from a URL and optionally save it locally.\n",
    "\n",
    "  Parameters:\n",
    "  -----------\n",
    "  url: str\n",
    "      URL of the PDF\n",
    "  save_path: Optional[str]\n",
    "      Optional local path to save the PDF\n",
    "\n",
    "  Returns:\n",
    "  --------\n",
    "  PDF content as bytes\n",
    "  \"\"\"\n",
    "  logger.info(f\"Fetching PDF from: {url}\")\n",
    "  response = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "  response.raise_for_status()\n",
    "\n",
    "  pdf_content = response.content\n",
    "  logger.info(f\"PDF downloaded: {len(pdf_content)} bytes\")\n",
    "\n",
    "  if save_path:\n",
    "      with open(save_path, \"wb\") as f:\n",
    "          f.write(pdf_content)\n",
    "      logger.info(f\"PDF saved to: {save_path}\")\n",
    "\n",
    "  return pdf_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gefmNEviVMHf",
    "outputId": "af7f881c-aa9e-44df-cd91-be99592cb0f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-17 22:25:43,463 - INFO - Fetching PDF from: https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n",
      "2025-10-17 22:25:45,504 - INFO - PDF downloaded: 569417 bytes\n",
      "2025-10-17 22:25:45,505 - INFO - PDF saved to: file.pdf\n"
     ]
    }
   ],
   "source": [
    "pdf_content = fetch_pdf_from_url(pdf_url, \"file.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z24rMISHVdIO"
   },
   "source": [
    "### **Extract text and images from PDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kaPRIzw4VPKr"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import pymupdf\n",
    "from PIL import Image\n",
    "\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GX4cIdQdVd-k"
   },
   "outputs": [],
   "source": [
    "def extract_content_from_pdf(pdf_content: bytes) -> Tuple[List[dict], List[dict]]:\n",
    "  \"\"\"\n",
    "  Extract all text blocks and images from PDF.\n",
    "\n",
    "  Parameters:\n",
    "  -----------\n",
    "  pdf_content: bytes\n",
    "      PDF file as bytes\n",
    "\n",
    "  Returns:\n",
    "  --------\n",
    "  Tuple of lists of dictionaries containing text and images\n",
    "  \"\"\"\n",
    "  pdf_document = pymupdf.open(stream=pdf_content, filetype=\"pdf\")\n",
    "\n",
    "  text_blocks = []\n",
    "  images = []\n",
    "  for page_num in range(len(pdf_document)):\n",
    "      page = pdf_document[page_num]\n",
    "      text = page.get_text()\n",
    "      image_list = page.get_images(full=True)\n",
    "\n",
    "      try:\n",
    "          if not isinstance(text, str):\n",
    "              continue\n",
    "\n",
    "          if text.strip():\n",
    "                  text_blocks.append(\n",
    "                      {\"page\": page_num + 1, \"text\": text.strip(), \"type\": \"text\"}\n",
    "                  )\n",
    "      except Exception as e:\n",
    "          logger.info(f\"Could not process text on page {page_num + 1}: {e}\")\n",
    "\n",
    "      for img_index, img_info in enumerate(image_list):\n",
    "          xref = img_info[0]\n",
    "          base_image = pdf_document.extract_image(xref)\n",
    "          image_bytes = base_image[\"image\"]\n",
    "\n",
    "          try:\n",
    "              image = Image.open(io.BytesIO(image_bytes))\n",
    "\n",
    "              images.append(\n",
    "                  {\n",
    "                      \"page\": page_num + 1,\n",
    "                      \"image\": image,\n",
    "                      \"type\": \"image\",\n",
    "                      \"index\": img_index,\n",
    "                  }\n",
    "              )\n",
    "          except Exception as e:\n",
    "              logger.info(f\"Could not process image on page {page_num + 1}: {e}\")\n",
    "\n",
    "  pdf_document.close()\n",
    "  logger.info(f\"Extracted {len(text_blocks)} text blocks and {len(images)} images.\")\n",
    "\n",
    "  return text_blocks, images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gn4PCgUDVz9W",
    "outputId": "e7d3ce42-da39-4bba-defb-526c69210236"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-17 22:25:45,846 - INFO - Extracted 11 text blocks and 3 images.\n"
     ]
    }
   ],
   "source": [
    "text_blocks, images = extract_content_from_pdf(pdf_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KIM6yZY6V0ru",
    "outputId": "90e1e9db-d4b0-4e1b-8b6a-4681d06cae01"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page': 1,\n",
       "  'text': 'Attention Is All You Need\\nAshish Vaswani\u2217\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer\u2217\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar\u2217\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit\u2217\\nGoogle Research\\nusz@google.com\\nLlion Jones\u2217\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez\u2217\u2020\\nUniversity of Toronto\\naidan@cs.toronto.edu\\n\u0141ukasz Kaiser\u2217\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin\u2217\u2021\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signi\ufb01cantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\n1\\nIntroduction\\nRecurrent neural networks, long short-term memory [12] and gated recurrent [7] neural networks\\nin particular, have been \ufb01rmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [29, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the \ufb01rst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nef\ufb01cient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n\u2020Work performed while at Google Brain.\\n\u2021Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.',\n",
       "  'type': 'text'},\n",
       " {'page': 2,\n",
       "  'text': 'Recurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht\u22121 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsigni\ufb01cant improvements in computational ef\ufb01ciency through factorization tricks [18] and conditional\\ncomputation [26], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for signi\ufb01cantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2\\nBackground\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more dif\ufb01cult to learn dependencies between distant positions [11]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 22, 23, 19].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [28].\\nTo the best of our knowledge, however, the Transformer is the \ufb01rst transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [14, 15] and [8].\\n3\\nModel Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[9], consuming the previously generated symbols as additional input when generating the next.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1\\nEncoder and Decoder Stacks\\nEncoder:\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The \ufb01rst is a multi-head self-attention mechanism, and the second is a simple, position-\\n2',\n",
       "  'type': 'text'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_blocks[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "61MPx_XcV1QC",
    "outputId": "6fcff279-4f04-4f65-8998-f06fa66d6f26"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page': 3,\n",
       "  'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=1520x2239>,\n",
       "  'type': 'image',\n",
       "  'index': 0},\n",
       " {'page': 4,\n",
       "  'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=835x1282>,\n",
       "  'type': 'image',\n",
       "  'index': 0},\n",
       " {'page': 4,\n",
       "  'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=445x884>,\n",
       "  'type': 'image',\n",
       "  'index': 1}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qRq94vvYV-Qp"
   },
   "source": [
    "### **Embedding text and images**\n",
    "\n",
    "`jina-clip-v1` is a state-of-the-art English **multimodal (text-image) embedding model**. [Read more](https://huggingface.co/jinaai/jina-clip-v1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SGNL9M71V8fa"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pfc_RJn9WBui"
   },
   "outputs": [],
   "source": [
    "def generate_content_embeddings(model, text_blocks: List[dict], images: List[dict]):\n",
    "  \"\"\"\n",
    "  Generate embeddings for text blocks and images using embedding model.\n",
    "\n",
    "  Parameters:\n",
    "  -----------\n",
    "  model: model\n",
    "  text_blocks: List of text block dictionaries\n",
    "  images: List of images dictionaries\n",
    "\n",
    "  Returns:\n",
    "  --------\n",
    "  Tuple of np.array of embeddings or None\n",
    "  \"\"\"\n",
    "\n",
    "  text_embeddings = None\n",
    "  images_embeddings = None\n",
    "\n",
    "  if text_blocks:\n",
    "      logger.info(f\"Encoding {len(text_blocks)} text blocks...\")\n",
    "      text_contents = [block[\"text\"] for block in text_blocks]\n",
    "      text_embeddings = model.encode_text(text_contents)\n",
    "      logger.info(f\"Text embeddings shape {text_embeddings.shape}\")\n",
    "\n",
    "  if images:\n",
    "      logger.info(f\"Encoding {len(images)} images...\")\n",
    "      image_contents = [img[\"image\"] for img in images]\n",
    "      images_embeddings = model.encode_image(image_contents)\n",
    "      logger.info(f\"Image embeddings shape {images_embeddings.shape}\")\n",
    "\n",
    "  return text_embeddings, images_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "b46dd100fb23493fa46100b6ff162ef9",
      "6b2536f75bd64d95949b05ea8e1b3ecc",
      "bccc4fd4271f44139b1065b1d5756df2",
      "6b0020d9c8594467baf9be1ff8f9b4b8",
      "33163475a9b747cb862f987b589b0c64",
      "58d86322cd86419ea8356ea7ec3422a8",
      "87c028ab395a4226b31fa8498d9b85aa",
      "012d71d152924eb5a79a35b0fff54ae1",
      "a657e97fc8064190a04e39a7f4dee057",
      "9f5207650ef742589d440bb4af54ce18",
      "c9c2714fb91b485dab08295a75973ea1",
      "f6d7c2f95823488dbbd6dedbb85c11aa",
      "b5e4c2dfc1a8401d8807e8ef2ee51d39",
      "f816de24193c44ea88f0f12c67f7dacd",
      "9ec47c2a938547ef855d12c0e4997cc6",
      "5756405cdb4f4424b4ec63095cfdb58b",
      "293b81e60fd4434eae2db261db16b4f3",
      "fccbbe1d1371483f8fc4953551fbfa3e",
      "3ef1308726be40218012f72522c16688",
      "5a547da929934749ba9b5d095f160806",
      "5a2e62616259420fb8e6188f2b5b1c6a",
      "07da2fb11d9446439121c4290d85ea45",
      "b3cf9a4f41914ce1900d987cb36e71be",
      "cc911a491326458eb2e9b5361ddf77f3",
      "290e6e101f79496bb1ce53f72a979996",
      "5b8918e546c9452da8b8950301585bca",
      "26fdbd5bed3843ab9cfad11243943172",
      "863b496bec504fd59ca9b815fea4ea2a",
      "34e21549e41449a0bca41aca03e7b70c",
      "54c855f5d35644b5b96f0aff1f0257c7",
      "604d022652b24b298b1fbb647ba5b5d4",
      "3ee34c0ab600443db85dc4295c2b0951",
      "c80bb0167f1e4a54ad63160469486689",
      "670d871dd0a249ccac49bf1387a2fd85",
      "ca845b44e2ad458daf79a7d1d7d876c2",
      "3b65f517406246c2b41852c08a922639",
      "d673ec5ee5d4449aa496b4a91a62e60c",
      "ca0b213d11fa4f48b3422e9042f55c40",
      "6d47be316ab04275ba0ec6922ac0c830",
      "de1d275c2b734bd6b5a0b00581b9ab89",
      "028348ae382b4522bee20a1b00acceb3",
      "c782866fb17f45ae8ea04a786b20a4c3",
      "55da9c446c834a9597aa1c99266644cf",
      "60236d74b6a248a19374500c87ebc2a2",
      "0cbaaea11876458c9073862354d12bcd",
      "436b4a46a9f44a9bbd8c1da595e9386d",
      "cf464e227b54439fbd63ea52fd1f4d7a",
      "bb9e8ef1fe9346eb9522192959d0073d",
      "35f01795710846439c7fbf000899ecab",
      "40d5722b803b478cb5dfc7085834bcce",
      "c05ee8523df94bd6b34835bd80f1bbeb",
      "b0e7347092784471a2ef65407c4a4dcf",
      "57e19cb355424c58b9cefe4ede1729d7",
      "fa31cb340f9e4207ad896c5c8dd88f78",
      "677f65b40ac945d08be4262d64e5109f",
      "5143177ec8dc4972bd04b8562652739d",
      "d4d9712d37be44caae6974d1c902274b",
      "477a91e2ee9b421b8ae8b1641d89ebd3",
      "4bd4f019a93045e1b25bead80ae82b16",
      "0bbc4b0bb47a41b0ba16d728deebf164",
      "8e2d28a398134f58be44c6eb98cb63f6",
      "1f2111997f714817a8f8d5f3098a3d4c",
      "d3d5c274c9e4460a82a7d863c7164bf2",
      "ebe8e8fab12148b4ab68b991656f07bd",
      "ceb28afb13a54ccb8932f84bf226df06",
      "7e5ad779014246d3bdf3ee83102b24a6",
      "1cd94218659a41128c349d96df3f3687",
      "0ed6570befec4c90b438d90fda5b01a2",
      "ef586227b87b4f9da61e760b2a52b4f9",
      "c7005bca0a3d4eafa820064468a9a3aa",
      "79a11116687a4e66b0b1c17a0d063d5d",
      "92673edae1e946f289057c747e3a6b91",
      "d3e74a6081814d29867bc745a6977990",
      "21912f26329747f5b86b42239c5181a1",
      "9c00d78d3ac14beb8d116a61ba7e32e9",
      "9fb18a7341c94d68a83cead70ee4f3e1",
      "70fda7df037e4bc1bbef0ba0e926a123",
      "931091f121b347cc904dd71e6107efcb",
      "d95d9367b85b4dc6987a963d657b9f60",
      "fc19da1b3a384413846cf2d2212a313c",
      "4f33085542e04259ad89c51fe1d872aa",
      "5c856f0946a742b899f6c46498d16853",
      "0465e6761a2c46c3b674c6aee98f1b5b",
      "9c5abf862d2e4e1b8536745b36271bbc",
      "5a8e2ae133594afc9ad55fcc1205eb1c",
      "286a9dbb34e54ff2949363390e197fdf",
      "1f5c43feb8c241f5b06ab55f90ee1eb5",
      "53ad6f239fcd44bfb9a3f7f5efddce69",
      "af1c8f1b43474aaeb20d97489a2fd8fa",
      "0d2e63f6cf6a47f99a6d82ccbf7cafeb",
      "f803acc6a4b549d5a3730a101fdb0894",
      "21013393094047e1a5883586f816cde7",
      "2b95f42bc6d2452689f90e6aa75d7b85",
      "e728d6c0dfe44f46b69b612151f7066c",
      "6c943976fd4f41f080e2937b8e914b97",
      "c860d14617ee47949d797fc9ba32b109",
      "c99aa1f088e444178a72b32b70e31ed4",
      "07ce79bf82434e0584c765b7bba7efc0",
      "1bde4f2084b3492fa9e8995e3770174e",
      "500c01d6d15c42938e8b9d4d822cb1fc",
      "4bd2cb2e20254f6fb2c2e6336223b542",
      "af1e341ce6d54ec08fad21b57209087f",
      "d84cde55e8474021a9ea0df22b11ea81",
      "02476769efe147c69fc511028ebdae84",
      "6443cec4a6f440cf8fc9cdeaeb72c9b7",
      "cfe791f125154c1a9b4cd7a4bddd8c4a",
      "15320e8542f248c980b8ba1e1759abe5",
      "78020719ba7b4901a37010674bdda43f",
      "4d1f89096b924a1eaad35c8ef70c8e1e",
      "c6d2b07b57524cf3b833dbf157e1b8a5",
      "38aad950645b4b218e746037a06da2f6",
      "8c5250ca45614972a871cec8833a5019",
      "0c5b7bc696a945079e5c989bfa5d4d41",
      "d9e12510a79f46d9b8738b75a3aa3a6a",
      "42b010ebf8864d1682b82446aac736bf",
      "8ae6f40e33fa4648ae9aae2f1571980e",
      "fa758e17f2494b638eb715ff5e6a6f0e",
      "b1831f0f1b8e4e7d8c93a7c29c72e865",
      "0bb3aa6efc3a46b59443106de34edea4",
      "ad1c952295004bec9819dbee997af11e",
      "5188f576b3f147caafb0550a915caf43",
      "81d08a74b60b4f67920f8ce557639e58",
      "18dc29ee7a6346ed967b31bb39c303a2",
      "9aff9e47659f479da4f5da4998ae0f97",
      "75ac9356b3f242be9d9ed583a2e8e1fa",
      "2342e552b03c410583d6cce8e1067280",
      "28e001b39c9946a7b65437abd85bdc85",
      "39f0dede05ce419e8c2d89543a6614fb",
      "9f4cfde99f2c46c9b726b62e973f59d2",
      "a74c84b9b97f46ff948cac950a5ec574",
      "c5405fb339bb4c3fa6f8fe566c6802bc",
      "36eb17fdd95345e6806ea5957ef37b5e",
      "d3a6543eb28640b5bda54b319a2e4868",
      "d09c2dec23fd4fd5b09fb0ff312f7293",
      "f1ba8ef0686949a8bb53fa05e77f11f5",
      "0eb5e0f80b774d3eafd3e14af88287b4",
      "1614197a50d04870b61692774eb679e9",
      "1adbe45629e542a2856df1781c81fafb",
      "91e81aa8530e4df19977755920a442e7",
      "88a4ee6f2dd2471dabc374103889c12e",
      "22fe7afd8ad342129cca2d088e90ccc8",
      "e0420c5e62dd40ebb8447a6696f2a9ad",
      "c7d5b1c873ed45d59335b38917fd5e16",
      "dcb07ca90f174698a872c7af137c0fd1",
      "5dc6011d52674255ac05377d460024f1",
      "e737bc5459644a2ca24ffe978ff40420",
      "fcbd259f0d884b44838fc223d201c039",
      "a131fd0ed38b4661a91f307100f00506",
      "89c60895379b44aabc40099858f1200a",
      "d5603643100f4219a745a62945b1ccf5",
      "7a6025e0b80d4c1389b0c2d7be32de61",
      "a0e6e2074f9d4c8b8256d956ce928280",
      "0ad07f411d134b13bc92f9f6e643b5b5",
      "3a41a682326d4f21935f6f68f78590d7",
      "4348fc49582e4d498f7cb0978a9d4a4e",
      "15150d9f5fe24a9d91e67624650a4d1d",
      "dc938b8f31b84a56a724d4753723a26f",
      "d50b8d39a35a4e56b6d73aeecf26016e",
      "b5ca05836141423686955cd1b4e79726",
      "880bd944e0d74b93b56577003d9c7693",
      "f9a6e0b45395435088c3878be16f8e89",
      "3d28e8e6711f4f29b8145be2b5db50db",
      "a06d75efd7584bf9bd9b6c16c1c91af8",
      "866edbaf2e55442cb50f6eaf648e4682",
      "32f34b6a254e46b5ad3bd8257286fe23",
      "c3fce8f1b2a944d18355704b7daac284",
      "d8d499b8937b41a99bf49af57bcf1ff1",
      "a4a2c166e422482d9affe03560811df5",
      "2c35d38e8bf54f3f901b991d80408079",
      "827cbc4c2e6549268a1a76b579ebc031",
      "a9e40e1388f54fe8bb1db51dcab6f851",
      "648cc8603b3341eca395cac8d15f82f7",
      "629e7dad56c143d595b92fa610248021",
      "eddfb4259cf44cc8b6ad180155177009",
      "1fd504dbc1074302aa4b09a7d60e23fb",
      "26c534e1c1de4faf966dfd72a97fff61"
     ]
    },
    "id": "AkmMLNLYWLd8",
    "outputId": "17427198-9398-4a3c-e473-116511e71914"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b46dd100fb23493fa46100b6ff162ef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6d7c2f95823488dbbd6dedbb85c11aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_clip.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/jinaai/jina-clip-implementation:\n",
      "- configuration_clip.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3cf9a4f41914ce1900d987cb36e71be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_clip.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "670d871dd0a249ccac49bf1387a2fd85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "hf_model.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/jinaai/jina-clip-implementation:\n",
      "- hf_model.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cbaaea11876458c9073862354d12bcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "rope_embeddings.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/jinaai/jina-clip-implementation:\n",
      "- rope_embeddings.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5143177ec8dc4972bd04b8562652739d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "transform.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/jinaai/jina-clip-implementation:\n",
      "- transform.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cd94218659a41128c349d96df3f3687",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eva_model.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/jinaai/jina-clip-implementation:\n",
      "- eva_model.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/jinaai/jina-clip-implementation:\n",
      "- modeling_clip.py\n",
      "- hf_model.py\n",
      "- rope_embeddings.py\n",
      "- transform.py\n",
      "- eva_model.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "931091f121b347cc904dd71e6107efcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/891M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af1c8f1b43474aaeb20d97489a2fd8fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "500c01d6d15c42938e8b9d4d822cb1fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_bert.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/jinaai/jina-bert-flash-implementation:\n",
      "- configuration_bert.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38aad950645b4b218e746037a06da2f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_bert.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81d08a74b60b4f67920f8ce557639e58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "block.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3a6543eb28640b5bda54b319a2e4868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mlp.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/jinaai/jina-bert-flash-implementation:\n",
      "- mlp.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcb07ca90f174698a872c7af137c0fd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mha.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/jinaai/jina-bert-flash-implementation:\n",
      "- mha.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/jinaai/jina-bert-flash-implementation:\n",
      "- block.py\n",
      "- mlp.py\n",
      "- mha.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4348fc49582e4d498f7cb0978a9d4a4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "bert_padding.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/jinaai/jina-bert-flash-implementation:\n",
      "- bert_padding.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3fce8f1b2a944d18355704b7daac284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "embedding.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/jinaai/jina-bert-flash-implementation:\n",
      "- embedding.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/jinaai/jina-bert-flash-implementation:\n",
      "- modeling_bert.py\n",
      "- block.py\n",
      "- bert_padding.py\n",
      "- embedding.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    }
   ],
   "source": [
    "jina_model = AutoModel.from_pretrained(\"jinaai/jina-clip-v1\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368,
     "referenced_widgets": [
      "1712c3027883433098778342b61a9985",
      "48854913b9254feda93e57e6343cc37a",
      "b69fd9c1564448009b43c92a8a45e4d4",
      "56fb6e56596d4829ad3aa247338e53df",
      "6896709ee11b4e3db86f55f9f739da8d",
      "6c503115dbac4797b039525c6bd15e7a",
      "f6ae8f92267e4b29b71fd8a1d4bd616e",
      "be3c4c57ec6c415d9b4076f6c94c272e",
      "fead7bfda764470eba257f25c070eec3",
      "ba886534eb904126a6e3f245ee9a95e4",
      "43a5694ca1e14faeab10f768fa399e44",
      "a00f9f378f444ff0b12cca02c1f09599",
      "fcae3026e15f4eb885ae70e68654bb69",
      "1443d894739f435f97b254742c803e52",
      "686564db5a4d4b70ada0b14f8bdc4ca0",
      "932916e8c54e47a1b5333bbe558d6495",
      "2a544a710ce6480bb27945a7745c8371",
      "9a0ff4ae5f0447018d8abaf4c0452498",
      "f3a71ad5631b4ae0b9f12e9443fad11a",
      "4b868ec6afc34306a9f6568db16df37f",
      "e9390822cc4e483e86e1382ce4f81b5b",
      "2160115388504d72a7816631da59b885",
      "ec850b67bc7341f6999b9e051f1d2fbd",
      "6e91710a6581499ea41644bb43a64e50",
      "57b453a646c543acbf4edcf81a39a648",
      "828789ea83194628a70e244e3de255e2",
      "c28d64c5471340248bc3d47184cf55c8",
      "d77c4d9aeb644e04a7d34e6aca200437",
      "0c6aebbfb4634e02b23a00752fb66a59",
      "ddad7a8785cc42229ae6902b81d83105",
      "f989486652ef48aaba04ea900f02f165",
      "f1b76bb6704e49e586f20d3b889afe42",
      "e5425e81b46b40968fe01a155a75b4e5",
      "0d8bc8b8c29d43fc808cccb310c12652",
      "e58c7d19cac74c97b546449d0e62e91a",
      "56dcbf48de7b4327b504989ecc1c5861",
      "7dfa7f10352c4423aab61e93b6f71d41",
      "e334483a9c8f4101b764239b15d86473",
      "0014ebbfeb5b47aab7667dc203c04281",
      "76b7860744c04d6da3a371b2509e7384",
      "efb2b6b0524a4e84826190d4a8fc3f8d",
      "1605779d8dff4bb4b00274e8e4a3f5a3",
      "a0908de2b96649d5a73fc85f92f9ca23",
      "2867299e57b04bf3ad5154fb94ae2b5e",
      "4b897ff707b34162b8f3d1256ea7516c",
      "c6171110ad084ec2823080610c05a6f1",
      "47fb5abd066040e2966d7475be9e8715",
      "41cae440c63f4e88a86ed720984b39a0",
      "c49f86b3b9d94de5a12e4d6f46e18a37",
      "303815277179406b91a9149ab16755a0",
      "c6a05c959f76450faf22cfb0ce9b4b6b",
      "6d61e5b9e47a4454b67f97fca73b2bdd",
      "f4718023d67e445da1c65899a56e7d08",
      "3f090bd9bdd64a4a9c3f9bc79b95427a",
      "f847ad32836342668fc08b212f2458e5",
      "b981758333b24519b0900744b33d9cc0",
      "d513407c9bef4355ace14ef8ec06a2df",
      "cd2f1254a8d2440e9a5c809c57fb5563",
      "a2276d5073304e2e83b80a0e9cd9f00a",
      "c8de730dd3a94e28bf3ea3ed54c73d3c",
      "c89e3e2b75f4426d9091a59e61edf6b5",
      "7dacafc4a8fc470c899246b84dc7deef",
      "478e66291ecb4979b3c51cb8caa60fbe",
      "845a33c1ae66416492a5591b4d8f4335",
      "38da37fec54c480d85c0a8ab248f8373",
      "0d80b5d6a39446f1980ad0084ce98628"
     ]
    },
    "id": "MotjKmK-WNhp",
    "outputId": "9cf62935-6c98-427c-ab6c-370b3a9026c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-17 22:26:11,825 - INFO - Encoding 11 text blocks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1712c3027883433098778342b61a9985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a00f9f378f444ff0b12cca02c1f09599",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec850b67bc7341f6999b9e051f1d2fbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d8bc8b8c29d43fc808cccb310c12652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/695 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-17 22:26:17,815 - INFO - Text embeddings shape (11, 768)\n",
      "2025-10-17 22:26:17,816 - INFO - Encoding 3 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b897ff707b34162b8f3d1256ea7516c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/527 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b981758333b24519b0900744b33d9cc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processing_clip.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/jinaai/jina-clip-implementation:\n",
      "- processing_clip.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-17 22:26:18,785 - INFO - Image embeddings shape (3, 768)\n"
     ]
    }
   ],
   "source": [
    "text_embeddings, image_embeddings = generate_content_embeddings(jina_model, text_blocks, images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KytAIHk1YOOS",
    "outputId": "f525b8f8-c90b-458a-ac36-c1fc38e4123a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 768)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7A8EfwQRhRfD",
    "outputId": "dab1081b-8fa5-4eae-b44f-8d146bcc40ae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 768)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aEMWY91rtZ48"
   },
   "source": [
    "### **Retrieval database**\n",
    "\n",
    "Chroma is an open-source AI application database that simplifies building LLM applications by making knowledge, facts, and skills pluggable for LLMs. [Getting started](https://docs.trychroma.com/docs/overview/getting-started)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8sxZOgP9Yy6b"
   },
   "outputs": [],
   "source": [
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IeMbHbMbfNXq",
    "outputId": "5133fb18-bfbb-4520-a744-563f71673c83"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collection(name=rag)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chroma_client = chromadb.Client()\n",
    "\n",
    "collection = chroma_client.create_collection(\n",
    "    name=\"rag\", metadata={\"hnsw:space\": \"cosine\"}, get_or_create=True\n",
    ")\n",
    "\n",
    "collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3I8OCxeHfOFR"
   },
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ka7azxj4fO4N"
   },
   "outputs": [],
   "source": [
    "def generate_id(prefix: Any, index: Any):\n",
    "  \"\"\"Generate a unique ID using prefix and index.\"\"\"\n",
    "  return f\"{prefix}_{index}_{hash(str(index) + prefix) % 1000000}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "7h7g7-CVfPpy",
    "outputId": "19a6b7de-6897-42ca-8f52-6af380056a6f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'prefix_index_362238'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_id(\"prefix\", \"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HPaZTTPEhgr3"
   },
   "outputs": [],
   "source": [
    "def store_text_embeddings(collection, text_blocks, embeddings, source):\n",
    "    \"\"\"\n",
    "    Store text embeddings in ChromaDB.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    collection: ClientAPI\n",
    "        ChromaDB collection\n",
    "    text_blocks: List[dict]\n",
    "        List of text block dictionaries\n",
    "    embeddings: Text embeddings\n",
    "    source: str\n",
    "        Source identifier (URL or path)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    List of stored document IDs\n",
    "    \"\"\"\n",
    "    if embeddings is None or len(text_blocks) == 0:\n",
    "        logger.info(\"No text embeddings to store\")\n",
    "        return []\n",
    "\n",
    "    logger.info(f\"Storing {len(text_blocks)} text embeddings...\")\n",
    "\n",
    "    ids = []\n",
    "    embedding_list = []\n",
    "    documents = []\n",
    "    metadatas = []\n",
    "\n",
    "    for idx, (block, embedding) in enumerate(zip(text_blocks, embeddings)):\n",
    "        doc_id = generate_id(\"text\", idx)\n",
    "        ids.append(doc_id)\n",
    "\n",
    "        if torch.is_tensor(embedding):\n",
    "            embedding_list.append(embedding.cpu().numpy().tolist())\n",
    "        elif isinstance(embedding, np.ndarray):\n",
    "            embedding_list.append(embedding.tolist())\n",
    "\n",
    "        documents.append(block[\"text\"])\n",
    "        metadatas.append({\"type\": \"text\", \"page\": block[\"page\"], \"source\": source})\n",
    "\n",
    "    collection.add(\n",
    "        ids=ids, embeddings=embedding_list, documents=documents, metadatas=metadatas\n",
    "    )\n",
    "\n",
    "    logger.info(f\"Stored {len(ids)} text embeddings\")\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5X_lXg4xhjHy",
    "outputId": "03835464-6310-4f9d-d332-34dff5576dc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-17 22:26:21,144 - INFO - Storing 11 text embeddings...\n",
      "2025-10-17 22:26:21,168 - INFO - Stored 11 text embeddings\n"
     ]
    }
   ],
   "source": [
    "text_ids = store_text_embeddings(collection, text_blocks, text_embeddings, pdf_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bvHvvWIdhkIF",
    "outputId": "b64b935f-ae8c-4d8f-8707-f00c9268eee2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text_0_314840',\n",
       " 'text_1_355765',\n",
       " 'text_2_222606',\n",
       " 'text_3_678200',\n",
       " 'text_4_911187',\n",
       " 'text_5_353922',\n",
       " 'text_6_33269',\n",
       " 'text_7_578329',\n",
       " 'text_8_853131',\n",
       " 'text_9_235545',\n",
       " 'text_10_734797']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ctI2RVMxhlKy"
   },
   "outputs": [],
   "source": [
    "def store_image_embeddings(collection, images, embeddings, source):\n",
    "    \"\"\"\n",
    "    Store image embeddings in ChromaDB.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    collection: ChromaDB collection\n",
    "    images: List of image dictionaries\n",
    "    embeddings: Image embeddings\n",
    "    source: Source identifier (URL or path)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    List of stored document IDs\n",
    "    \"\"\"\n",
    "    if embeddings is None or len(images) == 0:\n",
    "        logger.info(\"No image embeddings to store\")\n",
    "        return []\n",
    "\n",
    "    logger.info(f\"Storing {len(images)} image embeddings...\")\n",
    "\n",
    "    ids = []\n",
    "    embedding_list = []\n",
    "    documents = []\n",
    "    metadatas = []\n",
    "\n",
    "    for idx, (img, embedding) in enumerate(zip(images, embeddings)):\n",
    "        doc_id = generate_id(\"image\", idx)\n",
    "        ids.append(doc_id)\n",
    "\n",
    "        if torch.is_tensor(embedding):\n",
    "            embedding_list.append(embedding.cpu().numpy().tolist())\n",
    "        elif isinstance(embedding, np.ndarray):\n",
    "            embedding_list.append(embedding.tolist())\n",
    "\n",
    "        documents.append(f\"Image from page {img['page']}, index {img['index']}\")\n",
    "        metadatas.append(\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"page\": img[\"page\"],\n",
    "                \"image_index\": img[\"index\"],\n",
    "                \"source\": source,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    collection.add(\n",
    "        ids=ids, embeddings=embedding_list, documents=documents, metadatas=metadatas\n",
    "    )\n",
    "\n",
    "    logger.info(f\"Stored {len(ids)} image embeddings\")\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OdwXBzpqhl6s",
    "outputId": "b004171c-23bb-42f9-f9cc-12df01585e04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-17 22:26:21,185 - INFO - Storing 3 image embeddings...\n",
      "2025-10-17 22:26:21,192 - INFO - Stored 3 image embeddings\n"
     ]
    }
   ],
   "source": [
    "image_ids = store_image_embeddings(collection, images, image_embeddings, pdf_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NcqyU_fwho4L",
    "outputId": "01da7d30-1a45-46d9-c8bb-df63c77d49d7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['image_0_79277', 'image_1_715025', 'image_2_973616']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z2nbdSy8hnIf",
    "outputId": "ea6439cb-3733-4ca7-a290-72571d679a16"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BmluNdmhtxqU"
   },
   "source": [
    "### **Retrieve data from embeddings database**\n",
    "\n",
    "Transform a natural language query into an embedding, then perform similarity search on the Chroma database to retrieve relevant content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S05v7dZGhoGm"
   },
   "outputs": [],
   "source": [
    "def query_with_text(model, collection, query_text, n_results=5, filter_type=None):\n",
    "  \"\"\"\n",
    "  Query the database using text.\n",
    "\n",
    "  Parameters:\n",
    "  -----------\n",
    "  model: Jina-CLIP model\n",
    "  collection: ChromaDB collection\n",
    "  query_text: Query string\n",
    "  n_results: Number of results to return\n",
    "  filter_type: Optional filter ('text' or 'image')\n",
    "\n",
    "  Returns:\n",
    "  --------\n",
    "  Query results dictionary\n",
    "  \"\"\"\n",
    "  logger.info(f\"Querying with text: '{query_text}'\")\n",
    "\n",
    "  query_embedding = model.encode_text([query_text])[0]\n",
    "\n",
    "  if torch.is_tensor(query_embedding):\n",
    "      query_embedding_list = query_embedding.cpu().numpy().tolist()\n",
    "  elif isinstance(query_embedding, np.ndarray):\n",
    "      query_embedding_list = query_embedding.tolist()\n",
    "\n",
    "  where_filter = {\"type\": filter_type} if filter_type else None\n",
    "\n",
    "  results = collection.query(\n",
    "      query_embeddings=[query_embedding_list], n_results=n_results, where=where_filter\n",
    "  )\n",
    "\n",
    "  logger.info(f\"Found {len(results['ids'][0])} results\")\n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I15_I3eEhuLE",
    "outputId": "817e0750-4a9c-422b-da20-765a7367c73e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-17 22:26:21,218 - INFO - Querying with text: 'attention mechanism'\n",
      "2025-10-17 22:26:21,267 - INFO - Found 5 results\n"
     ]
    }
   ],
   "source": [
    "query_results = query_with_text(\n",
    "    model=jina_model,\n",
    "    collection=collection,\n",
    "    query_text=\"attention mechanism\",\n",
    "    n_results=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EKy6imvHinbZ",
    "outputId": "8b8da116-2a5a-4854-f368-b42a30763d61"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['text_3_678200',\n",
       "   'text_0_314840',\n",
       "   'text_4_911187',\n",
       "   'text_1_355765',\n",
       "   'text_10_734797']],\n",
       " 'embeddings': None,\n",
       " 'documents': [['Scaled Dot-Product Attention\\nMulti-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nquery with all keys, divide each by \u221adk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V ) = softmax(QKT\\n\u221adk\\n)V\\n(1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof\\n1\\n\u221adk . Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-ef\ufb01cient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by\\n1\\n\u221adk .\\n3.2.2\\nMulti-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it bene\ufb01cial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\noutput values. These are concatenated and once again projected, resulting in the \ufb01nal values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q \u00b7 k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4',\n",
       "   'Attention Is All You Need\\nAshish Vaswani\u2217\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer\u2217\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar\u2217\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit\u2217\\nGoogle Research\\nusz@google.com\\nLlion Jones\u2217\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez\u2217\u2020\\nUniversity of Toronto\\naidan@cs.toronto.edu\\n\u0141ukasz Kaiser\u2217\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin\u2217\u2021\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signi\ufb01cantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\n1\\nIntroduction\\nRecurrent neural networks, long short-term memory [12] and gated recurrent [7] neural networks\\nin particular, have been \ufb01rmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [29, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the \ufb01rst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nef\ufb01cient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n\u2020Work performed while at Google Brain.\\n\u2021Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.',\n",
       "   'MultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )\\nWhere the projections are parameter matrices W Q\\ni\\n\u2208Rdmodel\u00d7dk, W K\\ni\\n\u2208Rdmodel\u00d7dk, W V\\ni\\n\u2208Rdmodel\u00d7dv\\nand W O \u2208Rhdv\u00d7dmodel.\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3\\nApplications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n\u2022 In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[31, 2, 8].\\n\u2022 The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n\u2022 Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation \ufb02ow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to \u2212\u221e) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3\\nPosition-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2\\n(2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4\\nEmbeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [24]. In the embedding layers, we multiply those weights by \u221admodel.\\n3.5\\nPositional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\n5',\n",
       "   'Recurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht\u22121 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsigni\ufb01cant improvements in computational ef\ufb01ciency through factorization tricks [18] and conditional\\ncomputation [26], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for signi\ufb01cantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2\\nBackground\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more dif\ufb01cult to learn dependencies between distant positions [11]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 22, 23, 19].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [28].\\nTo the best of our knowledge, however, the Transformer is the \ufb01rst transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [14, 15] and [8].\\n3\\nModel Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[9], consuming the previously generated symbols as additional input when generating the next.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1\\nEncoder and Decoder Stacks\\nEncoder:\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The \ufb01rst is a multi-head self-attention mechanism, and the second is a simple, position-\\n2',\n",
       "   '[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n[22] Ankur Parikh, Oscar T\u00e4ckstr\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[24] O\ufb01r Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from over\ufb01tting. Journal of Machine\\nLearning Research, 15(1):1929\u20131958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440\u20132448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104\u20133112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\u2019s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n11']],\n",
       " 'uris': None,\n",
       " 'included': ['metadatas', 'documents', 'distances'],\n",
       " 'data': None,\n",
       " 'metadatas': [[{'page': 4,\n",
       "    'source': 'https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf',\n",
       "    'type': 'text'},\n",
       "   {'source': 'https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf',\n",
       "    'type': 'text',\n",
       "    'page': 1},\n",
       "   {'page': 5,\n",
       "    'type': 'text',\n",
       "    'source': 'https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf'},\n",
       "   {'page': 2,\n",
       "    'source': 'https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf',\n",
       "    'type': 'text'},\n",
       "   {'page': 11,\n",
       "    'source': 'https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf',\n",
       "    'type': 'text'}]],\n",
       " 'distances': [[0.5028226375579834,\n",
       "   0.5268853306770325,\n",
       "   0.5307607054710388,\n",
       "   0.5476062297821045,\n",
       "   0.6108148097991943]]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BnNWM_jEioJj",
    "outputId": "2372b8ea-617a-4603-bf30-8b10669c85e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ids', 'embeddings', 'documents', 'uris', 'included', 'data', 'metadatas', 'distances'])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6pssb3lbistl",
    "outputId": "a2023f8e-ff74-453e-8ee2-8d886fc1e6d3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Scaled Dot-Product Attention\\nMulti-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nquery with all keys, divide each by \u221adk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V ) = softmax(QKT\\n\u221adk\\n)V\\n(1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof\\n1\\n\u221adk . Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-ef\ufb01cient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by\\n1\\n\u221adk .\\n3.2.2\\nMulti-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it bene\ufb01cial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\noutput values. These are concatenated and once again projected, resulting in the \ufb01nal values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q \u00b7 k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4',\n",
       " 'Attention Is All You Need\\nAshish Vaswani\u2217\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer\u2217\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar\u2217\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit\u2217\\nGoogle Research\\nusz@google.com\\nLlion Jones\u2217\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez\u2217\u2020\\nUniversity of Toronto\\naidan@cs.toronto.edu\\n\u0141ukasz Kaiser\u2217\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin\u2217\u2021\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signi\ufb01cantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\n1\\nIntroduction\\nRecurrent neural networks, long short-term memory [12] and gated recurrent [7] neural networks\\nin particular, have been \ufb01rmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [29, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the \ufb01rst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nef\ufb01cient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n\u2020Work performed while at Google Brain.\\n\u2021Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.',\n",
       " 'MultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )\\nWhere the projections are parameter matrices W Q\\ni\\n\u2208Rdmodel\u00d7dk, W K\\ni\\n\u2208Rdmodel\u00d7dk, W V\\ni\\n\u2208Rdmodel\u00d7dv\\nand W O \u2208Rhdv\u00d7dmodel.\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3\\nApplications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n\u2022 In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[31, 2, 8].\\n\u2022 The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n\u2022 Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation \ufb02ow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to \u2212\u221e) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3\\nPosition-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2\\n(2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4\\nEmbeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [24]. In the embedding layers, we multiply those weights by \u221admodel.\\n3.5\\nPositional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\n5',\n",
       " 'Recurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht\u22121 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsigni\ufb01cant improvements in computational ef\ufb01ciency through factorization tricks [18] and conditional\\ncomputation [26], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for signi\ufb01cantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2\\nBackground\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more dif\ufb01cult to learn dependencies between distant positions [11]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 22, 23, 19].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [28].\\nTo the best of our knowledge, however, the Transformer is the \ufb01rst transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [14, 15] and [8].\\n3\\nModel Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[9], consuming the previously generated symbols as additional input when generating the next.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1\\nEncoder and Decoder Stacks\\nEncoder:\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The \ufb01rst is a multi-head self-attention mechanism, and the second is a simple, position-\\n2',\n",
       " '[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n[22] Ankur Parikh, Oscar T\u00e4ckstr\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[24] O\ufb01r Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from over\ufb01tting. Journal of Machine\\nLearning Research, 15(1):1929\u20131958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440\u20132448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104\u20133112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\u2019s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n11']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_results['documents'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v7MtvSJJt5RT"
   },
   "source": [
    "### **Contextualized generative step**\n",
    "\n",
    "This section integrates the retrieval system with content generation capabilities based on user queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-1llxu7E55Y"
   },
   "source": [
    "### **Context formatting**\n",
    "\n",
    "Format the context following examples from the official [Phi-3-vision-128k-instruct](https://huggingface.co/microsoft/Phi-3-vision-128k-instruct) documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3vnWGRzritel"
   },
   "outputs": [],
   "source": [
    "def format_context_for_phi3(results, max_context_length=None):\n",
    "  \"\"\"\n",
    "  Format retrieved results into a context string for PHI-3.\n",
    "\n",
    "  Parameters:\n",
    "  -----------\n",
    "  results: Results from ChromaDB query\n",
    "  max_context_length: Maximum length of context to include\n",
    "\n",
    "  Returns:\n",
    "  --------\n",
    "  Formatted context string\n",
    "  \"\"\"\n",
    "  if not results or not results[\"ids\"][0]:\n",
    "      return \"No relevant context found.\"\n",
    "\n",
    "  context_parts = []\n",
    "  current_length = 0\n",
    "\n",
    "  for i in range(len(results[\"ids\"][0])):\n",
    "      doc = results[\"documents\"][0][i]\n",
    "      metadata = results[\"metadatas\"][0][i]\n",
    "      distance = results[\"distances\"][0][i]\n",
    "\n",
    "      if metadata[\"type\"] == \"text\":\n",
    "          context_piece = f\"[Text from page {metadata['page']}, relevance: {1-distance:.2f}]\\n{doc}\\n\"\n",
    "      else:\n",
    "          context_piece = f\"[Image from page {metadata['page']}, relevance: {1-distance:.2f}]\\n{doc}\\n\"\n",
    "\n",
    "      if max_context_length is not None:\n",
    "          if current_length + len(context_piece) > max_context_length:\n",
    "              break\n",
    "\n",
    "      context_parts.append(context_piece)\n",
    "      current_length += len(context_piece)\n",
    "\n",
    "  return \"\\n---\\n\".join(context_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oYvlkXOIiyvp",
    "outputId": "a3637811-0ff9-4e6f-9ebe-34e051a03684"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Text from page 4, relevance: 0.50]\n",
      "Scaled Dot-Product Attention\n",
      "Multi-Head Attention\n",
      "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\n",
      "attention layers running in parallel.\n",
      "query with all keys, divide each by \u221adk, and apply a softmax function to obtain the weights on the\n",
      "values.\n",
      "In practice, we compute the attention function on a set of queries simultaneously, packed together\n",
      "into a matrix Q. The keys and values are also packed together into matrices K and V . We compute\n",
      "the matrix of outputs as:\n",
      "Attention(Q, K, V ) = softmax(QKT\n",
      "\u221adk\n",
      ")V\n",
      "(1)\n",
      "The two most commonly used attention functions are additive attention [2], and dot-product (multi-\n",
      "plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\n",
      "of\n",
      "1\n",
      "\u221adk . Additive attention computes the compatibility function using a feed-forward network with\n",
      "a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\n",
      "m\n"
     ]
    }
   ],
   "source": [
    "context = format_context_for_phi3(query_results)\n",
    "print(context[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E6TSRyTgFEn6",
    "outputId": "2d59e1af-0876-4ff2-a666-d0c9c07a0501"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t\n"
     ]
    }
   ],
   "source": [
    "print(context[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TRqfpxE9Fbl6"
   },
   "source": [
    "### **Language model instance**\n",
    "\n",
    "Initialize the PHI-3 vision model for text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YRjn5OBkiz3J"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t85k7Kq5i2QW"
   },
   "outputs": [],
   "source": [
    "language_model_id = \"microsoft/Phi-3-vision-128k-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 847,
     "referenced_widgets": [
      "deca202202214a06b5d8492d5fc720fb",
      "9174f765a18a49279175724e4ed44db6",
      "79f8100e7498404f8aea640afeb73d40",
      "63229fc699ef42a99a0dc1f3da014a60",
      "e11ef9bde0894f458b82399852dfbe14",
      "078e49289b5d412f93e2370abf402062",
      "2f98bbdb4f944b1490e002c64b4e6d36",
      "994f19231c7644aabf1f1826c97e72bf",
      "f85b90af25e745f8b483e0422067aedf",
      "905d9532201c4e18ab76eaf40d8110d1",
      "48f0a8f3336943dd805b57106f21c7c6",
      "1faa02ace9324bbfb21d9029fa6c5658",
      "51bf1bdaaa3a4879bfd0e4f209833240",
      "5e317eff0d7b4711875992379128f395",
      "df4a3f0be0a84ea89602df1fc27948f7",
      "aab2177fab444f54b7dc52420ff9175c",
      "dfe2ae32018745e3aae47dc89e6a4c96",
      "4a6b4b37c88647069f00c746e6ca562d",
      "a612c74f310e40e49888eef43902fe79",
      "03951685183048dca85202951f9652b4",
      "6d453c9862644a8c88dd3bc165da8c9e",
      "fd9127032e614a5cad913ff94b133409",
      "280bb624be124968a9723a0c640f1357",
      "c884743396814edea728ddd178253893",
      "67c50f6fbd1345968aa93dd34e85eb68",
      "2c03dd358291464bbe46635bd1f6d3a6",
      "8d0d4570cb10414fa129e418fee4cfe7",
      "3855e18b9ebb4fc4b8a11c438578a891",
      "76b0d1b759b94dd9bec0425edc07acd3",
      "484ec22a97954fd6b07f3ee4f9603c6b",
      "6c586cd82b8140b8b6e8e1bbe7fa13ff",
      "c4eb95145e004508a27503c773b965b1",
      "00f1d00adac144dcaeb52f497c376d92",
      "dd0422d3eb804d3b94175b233a20dc97",
      "51b14a977ca54723961a2ff1dcd02281",
      "2cb0ac7dd04e4f09b8e60aae3cd11909",
      "5a74cc712e4f43a59961a126d27bfa9c",
      "4191a93046c6400aaef2f7c86d4b7ae6",
      "a71202f6eb1c47cab0ec178e852f3918",
      "b790c9613a3145eab3efee7584a902b4",
      "56d91e8a35d84c3d9618634dc2ad90ca",
      "5663db2c08de4d01b89e0472cbbe1d2f",
      "d5f451a959be4363bbbb43dc2d8467ec",
      "64d64e4f76f04b198c599b3b203c3e96",
      "f22f82d455f946eeb3e5d518ca220d30",
      "547c2c7df0554d64acfd9f1887ba1ab9",
      "06f7332a443146acad042d6a61413fb6",
      "92e771254ebe49249b6ba8ada2dc23a7",
      "336ef8d656f945fdb9dddf8aa9882b20",
      "c6c5577e3e4d42c68252945ec672f0c3",
      "2c465b21d6804c67b151ffdf4ec70204",
      "05835073d7fe47fa8085db00d6f59f65",
      "0ea69a3d14c54f08aa589847cc0b2048",
      "50acf49bf33347eb8900521fece34272",
      "c508b04041de49578a3a9712e30326f7",
      "17e2d074216646adbcf9d54e8f21a7e7",
      "83b7d7aabda94654a165f74d3f464fcc",
      "388b9279d6a3407889fe4e3227e3165e",
      "34a564bf9f7e49f69451ae87897e1ac6",
      "5dec5e41f45b40abb8bd0216cbfab59d",
      "517efca6da234b94a2e3d3852f5b46fd",
      "1952778554c7460d9be59de898a4c345",
      "fffef3c1e2b743daa6183ccfc2339cf7",
      "22d2d108f6564f78883cfd6c155ec9e2",
      "274240edc2ae45568203a66a941278ba",
      "6effabf0703046799d8ba632d3314c42",
      "7255c3b724f7401ab179825383f03ff0",
      "a71ac43fa0d043e299a8c0617056db14",
      "9514727ee8d44eaa9ed3f4605c7dce7d",
      "6809ce4774fc433ea7b8172a6bb87139",
      "496313a3110c436c84ee88490652a2df",
      "7ac35b37fac046f0b0894e91b026b899",
      "02a0338dd82e44ad82dd199cf7d492bb",
      "5041c456eef6436199ab9eb9eb2e3e10",
      "8cee59f8747746b0bd258109495cc654",
      "d861680cc43b44a18d02a153ed10cded",
      "4f1bfdc82a7f4d7693c5886d481794db",
      "75805c97ea4c4a319a3ae42ad5eb0460",
      "8e6f74fc9404493c9a23aa2594b2998c",
      "6bc9668df1da429abbd9061e5dd1947e",
      "b6d9843673034b4888eb914ed65a4ace",
      "dffac1b6a34c44d19d1ca344215428cf",
      "e019aa6bdb6b4edf82c90865871e36a8",
      "8807a126050e4943badc8bc8c04b63e3",
      "78ecd8426fc54e62a2b088121776d071",
      "9e17a8adf74d43adad06ad40781e77af",
      "329179965c214c8a901181f7c1224c5a",
      "e8f5ea732808403481d573c82c4b485a",
      "c41701c76e3b424ca40ae690212169b6",
      "f87aa6b91e2e4154b9620526e4331735",
      "036e6d49574641fc810c165e519107fc",
      "5a0dde579f314cc58e0ea9adfd2ef241",
      "f5975e6ee1714979a88fe74a7e000056",
      "2855098148fb480c80c201dd90e8f46c",
      "86a0af82c7f2475d9fc729b75d7ee81a",
      "0f12fc37d05d442d9ba6ee3faaf67503",
      "f751f850b808414f904b1df936959e70",
      "e765d0d0e3eb4cba999e25ede1848423",
      "53bd2cb1931d4a2692563119bb4ac6c2",
      "593b98e6499b42ec935f757aff10224d",
      "e6b781c441db4203ba0973aabde83429",
      "5ff4219f48cb47efa4c5837d337bc78b",
      "6591dcf279184b7f90abcf8666da602e",
      "a6ba5871551c4dc0acf39ceecb550f29",
      "8800deeb8de943598341267c9f0bc18c",
      "66e7d4bae8e84931b18be279a64f0ab8",
      "e74f121c1a1946c180c9168c6cc224e0",
      "6d408027266a4c8cbacc35fd811cfa54",
      "ba4682bae4594b66b2176c45f064c185",
      "7bd251c2316543d5a3e0780cdc97ff49",
      "e55a8ce3b54540b3aa968aae4709f0cb",
      "1e15bba6ebf24ca6a058ada8fd37f302",
      "4b0484a52ce044949c032825b74545cf",
      "1c64703361f94dd0ac0f8ad9b08f8d8d",
      "4ab58aa79f0b412f892d27ea066424c0",
      "87565c5de7164f2c954958a108026493",
      "19f8adf0c39447649389febae3af868f",
      "312708ed63074df9814dbfe03d39e6a8",
      "78608c3e1b1c4ff28867dc1b05b3fffa",
      "509a97edfd1b4558b80d16402ecbbf45",
      "8e3872f19b4d4f54b2be68c0e56e63fe",
      "0309d7a930bc4f24a768f3075ec2d32c",
      "ffeec1f1f7a04053bd1e6f75422b9b3b",
      "6bb44ebaf31d4a0495e441e36b93401c",
      "a9444ec1c9a74827bdcff8c5884de671",
      "fb7ae1f698ee40ce8df18ba6ba8bcafb",
      "7f047bc272f14d0e9f4c2a46b266aaad",
      "fb4c921c5bab4aa2af453d4a2d96d13c",
      "deba06f4108848969626b73608851fea",
      "213164bc5bb541ea8408e127393cc911",
      "7f19ba76b7204368b44593c80ebc3d1a",
      "8149de196a734846acf726fc6371f7ef",
      "344415e2657145d892610d6de1485ab7",
      "f8e8e6ef607f494da3299f5c50e7522a",
      "54bbec22e42045b596efda3df330dfd3",
      "c41a8477b86d43e1a316d0642cbb8982",
      "4af598b513c744889d87bf0ede6e57d5",
      "3da41e178c644e3e8c57acbf651e81ec",
      "76a079651751462ba3a91891736cb4e3",
      "56656bdb84644143b42f10bf862a625a",
      "23a34f5966d74094bd31850e82aae07e",
      "f82432d27949455ebb5295738027285a",
      "254648811e0d4a4481c31aa4d691d948",
      "b0391ea3dfc44825b16750f84ef472b9",
      "f27d935c878c4d8ba05a3313c322f9ba",
      "8fab5a31bd494111b0d932c54a6f9188",
      "8be467102bb344f790861259fe5c67ff",
      "9007b75f12c443e1817b90faaf945be2",
      "ba7b2d89ead849d19a70dd8e4017febb",
      "e2a2a4ca162844da8b89273735df2366",
      "f34ef868ece14d3ea9a301734eca9d5e",
      "8d9bcef9543d4b44b6c9e084b9e89b84",
      "a5c82f095ce44475b7e396aff504823e",
      "ba7aaad50e904bd8bb57df1354cf0e9a",
      "2416cde8fa504ab9ba98ee503a09a175",
      "3d96ca324e924060bb88045514141dfa",
      "5fe80919a48c4a49bfd4f5545a0c566e",
      "39c0e3ea67a64ebbaea7580546ce4205",
      "b66901b05d1349dda42e1cd94cf6e8d3",
      "fdb011cc4ade4f038b5fac211143858f",
      "111a008d98f846558f1242e5898ecc78",
      "cffee13e88bb4727b9792ebb4f288b26",
      "b6e3ae7b4c0b4e52b89a753eb0277e0c",
      "d0245b5136e04382a0e13a4132d9dee6",
      "4e608fc07efb4f228ec8d6bfd06cacc0"
     ]
    },
    "id": "nSmpU_hNRXqZ",
    "outputId": "db81c38c-8734-41e5-a2a6-e3a44810b897"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deca202202214a06b5d8492d5fc720fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1faa02ace9324bbfb21d9029fa6c5658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_phi3_v.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-vision-128k-instruct:\n",
      "- configuration_phi3_v.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "280bb624be124968a9723a0c640f1357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_phi3_v.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd0422d3eb804d3b94175b233a20dc97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "image_embedding_phi3_v.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-vision-128k-instruct:\n",
      "- image_embedding_phi3_v.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-vision-128k-instruct:\n",
      "- modeling_phi3_v.py\n",
      "- image_embedding_phi3_v.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f22f82d455f946eeb3e5d518ca220d30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17e2d074216646adbcf9d54e8f21a7e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7255c3b724f7401ab179825383f03ff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75805c97ea4c4a319a3ae42ad5eb0460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.35G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c41701c76e3b424ca40ae690212169b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "593b98e6499b42ec935f757aff10224d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/464 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e55a8ce3b54540b3aa968aae4709f0cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processing_phi3_v.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0309d7a930bc4f24a768f3075ec2d32c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "image_processing_phi3_v.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-vision-128k-instruct:\n",
      "- image_processing_phi3_v.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-vision-128k-instruct:\n",
      "- processing_phi3_v.py\n",
      "- image_processing_phi3_v.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/models/auto/image_processing_auto.py:590: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "344415e2657145d892610d6de1485ab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0391ea3dfc44825b16750f84ef472b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2416cde8fa504ab9ba98ee503a09a175",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/670 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "phi3_model = AutoModelForCausalLM.from_pretrained(language_model_id, device_map=\"cuda\", trust_remote_code=True, torch_dtype=\"auto\", _attn_implementation=\"flash_attention_2\")\n",
    "\n",
    "phi3_processor = AutoProcessor.from_pretrained(language_model_id, trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EFqip4p4qbt9",
    "outputId": "54a23f01-d170-4003-ee9f-06d868bad252"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Phi3VForCausalLM(\n",
       "  (model): Phi3VModel(\n",
       "    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (vision_embed_tokens): Phi3ImageEmbedding(\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "      (wte): Embedding(32064, 3072, padding_idx=32000)\n",
       "      (img_processor): CLIPVisionModel(\n",
       "        (vision_model): CLIPVisionTransformer(\n",
       "          (embeddings): CLIPVisionEmbeddings(\n",
       "            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "            (position_embedding): Embedding(577, 1024)\n",
       "          )\n",
       "          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder): CLIPEncoder(\n",
       "            (layers): ModuleList(\n",
       "              (0-23): 24 x CLIPEncoderLayer(\n",
       "                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): CLIPMLP(\n",
       "                  (activation_fn): QuickGELUActivation()\n",
       "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (self_attn): CLIPAttentionFA2(\n",
       "                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (img_projection): Sequential(\n",
       "        (0): Linear(in_features=4096, out_features=3072, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=3072, out_features=3072, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x Phi3DecoderLayer(\n",
       "        (self_attn): Phi3FlashAttention2(\n",
       "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
       "          (rotary_emb): Phi3SuScaledRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Phi3MLP(\n",
       "          (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (activation_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Phi3RMSNorm()\n",
       "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_attention_layernorm): Phi3RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): Phi3RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi3_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wttz3SoIF8HH"
   },
   "source": [
    "### **Model generation**\n",
    "\n",
    "Generate responses using the language model without RAG context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uCJxTZNyFVbq"
   },
   "outputs": [],
   "source": [
    "def generate_response_with_phi3(\n",
    "  phi3_model, phi3_processor, prompt, context,\n",
    "  images=None, max_new_tokens=512):\n",
    "  \"\"\"\n",
    "  Generate a response using PHI-3 Vision model with RAG context.\n",
    "\n",
    "  Parameters:\n",
    "  -----------\n",
    "  phi3_model: PHI-3 Vision model\n",
    "  phi3_processor: PHI-3 processor\n",
    "  prompt: User prompt/question\n",
    "  context: Retrieved context from RAG\n",
    "  images: Optional list of PIL Images\n",
    "  max_new_tokens: Maximum number of tokens to generate\n",
    "\n",
    "  Returns:\n",
    "  --------\n",
    "  Generated text response\n",
    "  \"\"\"\n",
    "  messages = [\n",
    "      {\"role\": \"system\", \"content\": f\"You are a helpful AI assistant. Use the following context from documents to answer the user's question.\\nIf the context doesn't contain relevant information, say so clearly.\\n\\nContext:\\n{context}\"},\n",
    "      {\"role\": \"user\", \"content\": prompt}\n",
    "  ]\n",
    "\n",
    "  prompt_text = phi3_processor.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "  if images and len(images) > 0:\n",
    "    inputs = phi3_processor(\n",
    "        text=prompt_text,\n",
    "        images=images,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "  else:\n",
    "    inputs = phi3_processor(\n",
    "        text=prompt_text,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "  device = next(phi3_model.parameters()).device\n",
    "  inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v\n",
    "          for k, v in inputs.items()}\n",
    "\n",
    "  generation_args = {\n",
    "      \"max_new_tokens\": max_new_tokens,\n",
    "      \"temperature\": 0.7, # Using the temperature from the original function\n",
    "      \"top_p\": 0.9, # Using the top_p from the original function\n",
    "      \"do_sample\": True if 0.7 > 0 else False, # do_sample based on temperature\n",
    "  }\n",
    "\n",
    "  logger.info(\"Generating response...\")\n",
    "  with torch.no_grad():\n",
    "    generate_ids = phi3_model.generate(\n",
    "        **inputs,\n",
    "        eos_token_id=phi3_processor.tokenizer.eos_token_id,\n",
    "        **generation_args\n",
    "    )\n",
    "\n",
    "  # remove input tokens\n",
    "  generate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]\n",
    "  response = phi3_processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "\n",
    "  return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f0RJoiDQFtJn",
    "outputId": "2dc277e9-cf27-4db4-c8ec-1c8a46028421"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-17 22:45:40,074 - INFO - Generating response...\n"
     ]
    }
   ],
   "source": [
    "response = generate_response_with_phi3(\n",
    "      phi3_model,\n",
    "      phi3_processor,\n",
    "      \"what is the attention mechanism\",\n",
    "      context,\n",
    "      max_new_tokens=3\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "ocVJBjlRMpe0",
    "outputId": "473104e3-0faa-4142-c673-ef1f2ea7b71c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The attention mechanism'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6c159251"
   },
   "outputs": [],
   "source": [
    "query = \"how does the attention mechanism works\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "089ba688",
    "outputId": "3a49605c-4ffc-4739-f9eb-3927b49fde07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-17 22:45:40,470 - INFO - \n",
      "================================================================================\n",
      "2025-10-17 22:45:40,471 - INFO - Processing query: how does the attention mechanism works\n",
      "2025-10-17 22:45:40,472 - INFO - ================================================================================\n",
      "\n",
      "2025-10-17 22:45:40,472 - INFO - Step 1: Retrieving relevant context...\n",
      "2025-10-17 22:45:40,473 - INFO - Querying with text: 'how does the attention mechanism works'\n",
      "2025-10-17 22:45:40,519 - INFO - Found 5 results\n",
      "2025-10-17 22:45:40,519 - INFO - \n",
      "Step 2: Formatting context...\n",
      "2025-10-17 22:45:40,520 - INFO - Context length: 15345 characters\n",
      "2025-10-17 22:45:40,521 - INFO - \n",
      "Step 3: Generating response with PHI-3...\n",
      "2025-10-17 22:45:40,532 - INFO - Generating response...\n",
      "2025-10-17 22:46:12,845 - INFO - \n",
      "================================================================================\n",
      "2025-10-17 22:46:12,846 - INFO - COMPLETE\n",
      "2025-10-17 22:46:12,847 - INFO - ================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = rag_query_and_generate(\n",
    "  jina_model,\n",
    "  phi3_model,\n",
    "  phi3_processor,\n",
    "  collection,\n",
    "  query,\n",
    "  n_results=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5be96116",
    "outputId": "8c1ea61b-82d2-48a5-f84d-6c0252cfe886"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The attention mechanism is a method that allows a model to focus on different parts of the input sequence when generating the output sequence. It does this by assigning different weights to different parts of the input sequence, allowing the model to focus on the most relevant information for each step of the output sequence. In the context of the Transformer model, the attention mechanism is used in three ways:\n",
      "\n",
      "1. Encoder-decoder attention: In this case, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence, mimicking the typical encoder-decoder attention mechanisms found in sequence-to-sequence models.\n",
      "2. Self-attention: In a self-attention layer, all of the keys, values, and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.\n",
      "3. Decoder self-attention: Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. To prevent leftward information flow in the decoder and preserve the auto-regressive property, masking is used inside of scaled dot-product attention by setting to -\u221e all values in the input of the softmax that correspond to illegal connections.\n",
      "\n",
      "The attention mechanism is a key component of the Transformer model, as it allows the model to jointly attend to information from different representation subspaces at different positions. This is achieved by using a stack of multiple attention layers, each with its own set of learned linear projections and dot-product attention. The model then combines the outputs of these attention layers to produce the final output sequence.\n",
      "\n",
      "In addition to the attention mechanism, the Transformer model also uses position-wise feed-forward networks and learned embeddings to improve the model's ability to capture long-range dependencies and understand the context of the input sequence. The model also incorporates a masking mechanism to prevent the model from attending to positions that are too far away in the input sequence, which can improve the model's ability to capture local dependencies and understand the context of the input sequence.\n",
      "\n",
      "Overall, the attention mechanism in the Transformer model is\n"
     ]
    }
   ],
   "source": [
    "print(result[\"response\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7286u4HmFuI8"
   },
   "source": [
    "### **Model generation with RAG context**\n",
    "\n",
    "Generate responses using the language model enhanced with retrieved context from the RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MZxzyNmni4yY"
   },
   "outputs": [],
   "source": [
    "def rag_query_and_generate(\n",
    "  jina_model, phi3_model, phi3_processor, collection,\n",
    "  query_text, n_results=5, filter_type=None,\n",
    "  max_new_tokens=512):\n",
    "  \"\"\"\n",
    "  Complete RAG pipeline: retrieve relevant context and generate response.\n",
    "\n",
    "  Parameters:\n",
    "  -----------\n",
    "  jina_model: Jina-CLIP model for retrieval\n",
    "  phi3_model: PHI-3 Vision model for generation\n",
    "  phi3_processor: PHI-3 processor\n",
    "  collection: ChromaDB collection\n",
    "  query_text: User query\n",
    "  n_results: Number of results to retrieve\n",
    "  filter_type: Optional filter for retrieval\n",
    "  max_new_tokens: Maximum tokens to generate\n",
    "\n",
    "  Returns:\n",
    "  --------\n",
    "  Dictionary with results and generated response\n",
    "  \"\"\"\n",
    "  logger.info(f\"\\n{\"=\"*80}\")\n",
    "  logger.info(f\"Processing query: {query_text}\")\n",
    "  logger.info(f\"{\"=\"*80}\\n\")\n",
    "\n",
    "  logger.info(\"Step 1: Retrieving relevant context...\")\n",
    "  results = query_with_text(\n",
    "      jina_model, collection, query_text,\n",
    "      n_results=n_results, filter_type=filter_type)\n",
    "\n",
    "  logger.info(\"\\nStep 2: Formatting context...\")\n",
    "  context = format_context_for_phi3(results)\n",
    "  logger.info(f\"Context length: {len(context)} characters\")\n",
    "\n",
    "  logger.info(\"\\nStep 3: Generating response with PHI-3...\")\n",
    "  response = generate_response_with_phi3(\n",
    "      phi3_model,\n",
    "      phi3_processor,\n",
    "      query_text,\n",
    "      context,\n",
    "      max_new_tokens=max_new_tokens\n",
    "  )\n",
    "\n",
    "  logger.info(f\"\\n{\"=\"*80}\")\n",
    "  logger.info(\"COMPLETE\")\n",
    "  logger.info(f\"{\"=\"*80}\\n\")\n",
    "\n",
    "  return {\n",
    "      \"query\": query_text,\n",
    "      \"retrieved_results\": results,\n",
    "      \"context\": context,\n",
    "      \"response\": response\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xh2yb0c6jxsy"
   },
   "outputs": [],
   "source": [
    "query = \"how do the attention mechanism works\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AO7HtOvakjkd",
    "outputId": "b0c52920-e969-4e4c-befa-ba257fda2559"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-17 22:46:12,881 - INFO - \n",
      "================================================================================\n",
      "2025-10-17 22:46:12,882 - INFO - Processing query: how do the attention mechanism works\n",
      "2025-10-17 22:46:12,883 - INFO - ================================================================================\n",
      "\n",
      "2025-10-17 22:46:12,883 - INFO - Step 1: Retrieving relevant context...\n",
      "2025-10-17 22:46:12,884 - INFO - Querying with text: 'how do the attention mechanism works'\n",
      "2025-10-17 22:46:12,946 - INFO - Found 5 results\n",
      "2025-10-17 22:46:12,946 - INFO - \n",
      "Step 2: Formatting context...\n",
      "2025-10-17 22:46:12,947 - INFO - Context length: 15345 characters\n",
      "2025-10-17 22:46:12,948 - INFO - \n",
      "Step 3: Generating response with PHI-3...\n",
      "2025-10-17 22:46:12,960 - INFO - Generating response...\n",
      "2025-10-17 22:46:25,826 - INFO - \n",
      "================================================================================\n",
      "2025-10-17 22:46:25,827 - INFO - COMPLETE\n",
      "2025-10-17 22:46:25,827 - INFO - ================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = rag_query_and_generate(\n",
    "  jina_model,\n",
    "  phi3_model,\n",
    "  phi3_processor,\n",
    "  collection,\n",
    "  query,\n",
    "  n_results=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nH94tz3hkkPF",
    "outputId": "fa03dfd2-afa6-465a-e47d-eac133edc544"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['query', 'retrieved_results', 'context', 'response'])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "kdQITQCRoHj4",
    "outputId": "bda8e439-5099-40ab-da82-d79969569365"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'how do the attention mechanism works'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"query\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ajWQXfNsI-V",
    "outputId": "4d5259a1-429b-4bfd-be29-c95d7abb3b1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The attention mechanism in the context of Transformer models allows the model to focus on different parts of the input sequence when generating each part of the output sequence. It does this by computing a weighted sum of the values (from the input sequence) corresponding to each position in the output sequence. This weighted sum is then used to compute a representation for the current position in the output sequence. The weights are computed by applying a softmax function to the dot product of the query with all the keys. The query and keys are derived from the encoder and decoder stacks, respectively, and are linearly transformed using learned weight matrices. The resulting values are then used as input for the next layer in the decoder. This process is repeated for each position in the output sequence. The attention mechanism enables the model to learn which parts of the input sequence are more relevant for each part of the output sequence, allowing for more efficient and effective modeling of dependencies between input and output sequences.\n"
     ]
    }
   ],
   "source": [
    "print(result[\"response\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a0def51b"
   },
   "source": [
    "### **Interactive RAG Chat**\n",
    "\n",
    "Engage in a turn-based chat with the RAG system and the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ac9aaa9"
   },
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "\n",
    "def chat_with_rag(query, chat_history=[], n_results=3):\n",
    "    \"\"\"\n",
    "    Interactive chat function with RAG.\n",
    "    \"\"\"\n",
    "    chat_history.append({\"role\": \"user\", \"content\": query})\n",
    "\n",
    "    rag_response = rag_query_and_generate(\n",
    "        jina_model,\n",
    "        phi3_model,\n",
    "        phi3_processor,\n",
    "        collection,\n",
    "        query,\n",
    "        n_results=n_results\n",
    "    )\n",
    "\n",
    "    full_response = f\"Retrieved Context:\\n---\\n{rag_response['context']}\\n---\\n\\nGenerated Answer:\\n{rag_response['response']}\"\n",
    "\n",
    "    chat_history.append({\"role\": \"assistant\", \"content\": full_response})\n",
    "\n",
    "    print(\"\\nGenerated Answer:\")\n",
    "    print(rag_response['response'])\n",
    "\n",
    "    return full_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 558
    },
    "id": "Q_Zhk5tWN0Ub",
    "outputId": "fe61d67b-f0e3-426a-ebbf-a0f0f5e861ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-17 22:52:32,715 - INFO - \n",
      "================================================================================\n",
      "2025-10-17 22:52:32,715 - INFO - Processing query: What is the main architecture proposed in the 'Attention is All You Need' paper?\n",
      "2025-10-17 22:52:32,716 - INFO - ================================================================================\n",
      "\n",
      "2025-10-17 22:52:32,716 - INFO - Step 1: Retrieving relevant context...\n",
      "2025-10-17 22:52:32,717 - INFO - Querying with text: 'What is the main architecture proposed in the 'Attention is All You Need' paper?'\n",
      "2025-10-17 22:52:32,789 - INFO - Found 3 results\n",
      "2025-10-17 22:52:32,790 - INFO - \n",
      "Step 2: Formatting context...\n",
      "2025-10-17 22:52:32,790 - INFO - Context length: 8682 characters\n",
      "2025-10-17 22:52:32,791 - INFO - \n",
      "Step 3: Generating response with PHI-3...\n",
      "2025-10-17 22:52:32,800 - INFO - Generating response...\n",
      "2025-10-17 22:52:35,698 - INFO - \n",
      "================================================================================\n",
      "2025-10-17 22:52:35,699 - INFO - COMPLETE\n",
      "2025-10-17 22:52:35,699 - INFO - ================================================================================\n",
      "\n",
      "\n",
      "Generated Answer:\n",
      "The main architecture proposed in the 'Attention is All You Need' paper is the Transformer model, which is based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Retrieved Context:\\n---\\n[Text from page 1, relevance: 0.44]\\nAttention Is All You Need\\nAshish Vaswani\u2217\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer\u2217\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar\u2217\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit\u2217\\nGoogle Research\\nusz@google.com\\nLlion Jones\u2217\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez\u2217\u2020\\nUniversity of Toronto\\naidan@cs.toronto.edu\\n\u0141ukasz Kaiser\u2217\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin\u2217\u2021\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signi\ufb01cantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\n1\\nIntroduction\\nRecurrent neural networks, long short-term memory [12] and gated recurrent [7] neural networks\\nin particular, have been \ufb01rmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [29, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the \ufb01rst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nef\ufb01cient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n\u2020Work performed while at Google Brain.\\n\u2021Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\n\\n---\\n[Text from page 5, relevance: 0.33]\\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )\\nWhere the projections are parameter matrices W Q\\ni\\n\u2208Rdmodel\u00d7dk, W K\\ni\\n\u2208Rdmodel\u00d7dk, W V\\ni\\n\u2208Rdmodel\u00d7dv\\nand W O \u2208Rhdv\u00d7dmodel.\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3\\nApplications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n\u2022 In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[31, 2, 8].\\n\u2022 The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n\u2022 Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation \ufb02ow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to \u2212\u221e) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3\\nPosition-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2\\n(2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4\\nEmbeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [24]. In the embedding layers, we multiply those weights by \u221admodel.\\n3.5\\nPositional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\n5\\n\\n---\\n[Text from page 4, relevance: 0.30]\\nScaled Dot-Product Attention\\nMulti-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nquery with all keys, divide each by \u221adk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V ) = softmax(QKT\\n\u221adk\\n)V\\n(1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof\\n1\\n\u221adk . Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-ef\ufb01cient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by\\n1\\n\u221adk .\\n3.2.2\\nMulti-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it bene\ufb01cial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\noutput values. These are concatenated and once again projected, resulting in the \ufb01nal values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q \u00b7 k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4\\n\\n---\\n\\nGenerated Answer:\\nThe main architecture proposed in the \\'Attention is All You Need\\' paper is the Transformer model, which is based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_with_rag(\"What is the main architecture proposed in the 'Attention is All You Need' paper?\", chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 628
    },
    "id": "LnffZrpSN4VN",
    "outputId": "a869204e-fb64-4851-bcf2-fc6e4dd90b14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-17 22:52:35,705 - INFO - \n",
      "================================================================================\n",
      "2025-10-17 22:52:35,706 - INFO - Processing query: Explain the multi-head attention mechanism.\n",
      "2025-10-17 22:52:35,706 - INFO - ================================================================================\n",
      "\n",
      "2025-10-17 22:52:35,707 - INFO - Step 1: Retrieving relevant context...\n",
      "2025-10-17 22:52:35,707 - INFO - Querying with text: 'Explain the multi-head attention mechanism.'\n",
      "2025-10-17 22:52:35,774 - INFO - Found 3 results\n",
      "2025-10-17 22:52:35,774 - INFO - \n",
      "Step 2: Formatting context...\n",
      "2025-10-17 22:52:35,775 - INFO - Context length: 10028 characters\n",
      "2025-10-17 22:52:35,775 - INFO - \n",
      "Step 3: Generating response with PHI-3...\n",
      "2025-10-17 22:52:35,784 - INFO - Generating response...\n",
      "2025-10-17 22:52:53,346 - INFO - \n",
      "================================================================================\n",
      "2025-10-17 22:52:53,347 - INFO - COMPLETE\n",
      "2025-10-17 22:52:53,347 - INFO - ================================================================================\n",
      "\n",
      "\n",
      "Generated Answer:\n",
      "The multi-head attention mechanism is a component of the Transformer model that allows the model to jointly attend to information from different representation subspaces at different positions. In a single attention head, the queries, keys, and values are linearly projected to dk, dk, and dv dimensions, respectively, resulting in the \ufb01nal values. These are then concatenated and once again projected, resulting in the \ufb01nal values.\n",
      "\n",
      "In the multi-head attention mechanism, the queries, keys, and values are projected into several dk-dimensional subspaces, allowing the model to attend to information from different representation subspaces at different positions. Each attention head independently computes the dot products between the queries and keys, divides each by \u221adk, and applies a softmax function to obtain the weights on the values. These weights are then used to combine the values from each head, resulting in the final output.\n",
      "\n",
      "The multi-head attention mechanism allows for more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs. It has become an integral part of compelling sequence modeling and translation models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences.\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Retrieved Context:\\n---\\n[Text from page 5, relevance: 0.70]\\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )\\nWhere the projections are parameter matrices W Q\\ni\\n\u2208Rdmodel\u00d7dk, W K\\ni\\n\u2208Rdmodel\u00d7dk, W V\\ni\\n\u2208Rdmodel\u00d7dv\\nand W O \u2208Rhdv\u00d7dmodel.\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3\\nApplications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n\u2022 In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[31, 2, 8].\\n\u2022 The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n\u2022 Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation \ufb02ow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to \u2212\u221e) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3\\nPosition-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2\\n(2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4\\nEmbeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [24]. In the embedding layers, we multiply those weights by \u221admodel.\\n3.5\\nPositional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\n5\\n\\n---\\n[Text from page 4, relevance: 0.62]\\nScaled Dot-Product Attention\\nMulti-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nquery with all keys, divide each by \u221adk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V ) = softmax(QKT\\n\u221adk\\n)V\\n(1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof\\n1\\n\u221adk . Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-ef\ufb01cient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by\\n1\\n\u221adk .\\n3.2.2\\nMulti-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it bene\ufb01cial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\noutput values. These are concatenated and once again projected, resulting in the \ufb01nal values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q \u00b7 k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4\\n\\n---\\n[Text from page 2, relevance: 0.46]\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht\u22121 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsigni\ufb01cant improvements in computational ef\ufb01ciency through factorization tricks [18] and conditional\\ncomputation [26], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for signi\ufb01cantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2\\nBackground\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more dif\ufb01cult to learn dependencies between distant positions [11]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 22, 23, 19].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [28].\\nTo the best of our knowledge, however, the Transformer is the \ufb01rst transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [14, 15] and [8].\\n3\\nModel Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[9], consuming the previously generated symbols as additional input when generating the next.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1\\nEncoder and Decoder Stacks\\nEncoder:\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The \ufb01rst is a multi-head self-attention mechanism, and the second is a simple, position-\\n2\\n\\n---\\n\\nGenerated Answer:\\nThe multi-head attention mechanism is a component of the Transformer model that allows the model to jointly attend to information from different representation subspaces at different positions. In a single attention head, the queries, keys, and values are linearly projected to dk, dk, and dv dimensions, respectively, resulting in the \ufb01nal values. These are then concatenated and once again projected, resulting in the \ufb01nal values.\\n\\nIn the multi-head attention mechanism, the queries, keys, and values are projected into several dk-dimensional subspaces, allowing the model to attend to information from different representation subspaces at different positions. Each attention head independently computes the dot products between the queries and keys, divides each by \u221adk, and applies a softmax function to obtain the weights on the values. These weights are then used to combine the values from each head, resulting in the final output.\\n\\nThe multi-head attention mechanism allows for more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs. It has become an integral part of compelling sequence modeling and translation models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences.'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_with_rag(\"Explain the multi-head attention mechanism.\", chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 558
    },
    "id": "njDrTZcqRiGu",
    "outputId": "6a8d7c98-6cea-404a-cfd0-37ad7ccafbbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-17 22:52:53,355 - INFO - \n",
      "================================================================================\n",
      "2025-10-17 22:52:53,356 - INFO - Processing query: Describe the model architecture diagram shown in the paper. What are its main components?\n",
      "2025-10-17 22:52:53,356 - INFO - ================================================================================\n",
      "\n",
      "2025-10-17 22:52:53,357 - INFO - Step 1: Retrieving relevant context...\n",
      "2025-10-17 22:52:53,357 - INFO - Querying with text: 'Describe the model architecture diagram shown in the paper. What are its main components?'\n",
      "2025-10-17 22:52:53,450 - INFO - Found 3 results\n",
      "2025-10-17 22:52:53,451 - INFO - \n",
      "Step 2: Formatting context...\n",
      "2025-10-17 22:52:53,452 - INFO - Context length: 8746 characters\n",
      "2025-10-17 22:52:53,453 - INFO - \n",
      "Step 3: Generating response with PHI-3...\n",
      "2025-10-17 22:52:53,460 - INFO - Generating response...\n",
      "2025-10-17 22:53:03,667 - INFO - \n",
      "================================================================================\n",
      "2025-10-17 22:53:03,668 - INFO - COMPLETE\n",
      "2025-10-17 22:53:03,668 - INFO - ================================================================================\n",
      "\n",
      "\n",
      "Generated Answer:\n",
      "The model architecture diagram shown in the paper consists of two main components: the encoder and the decoder. Both components are composed of stacked layers. The encoder includes a stack of N = 6 identical layers, each having two sub-layers - a multi-head self-attention mechanism and a simple, position-wise, fully connected layer. The decoder, on the other hand, also consists of a stack of N = 6 identical layers, but with an additional self-attention mechanism. The encoder and decoder stacks are then connected by a residual connection around each of the sub-layers in the model. All sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512.\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Retrieved Context:\\n---\\n[Text from page 3, relevance: 0.37]\\nFigure 1: The Transformer - model architecture.\\nwise fully connected feed-forward network. We employ a residual connection [10] around each of\\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder:\\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2\\nAttention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1\\nScaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\n3\\n\\n---\\n[Text from page 9, relevance: 0.27]\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN\\ndmodel\\ndff\\nh\\ndk\\ndv\\nPdrop\\n\u03f5ls\\ntrain\\nPPL\\nBLEU\\nparams\\nsteps\\n(dev)\\n(dev)\\n\u00d7106\\nbase\\n6\\n512\\n2048\\n8\\n64\\n64\\n0.1\\n0.1\\n100K\\n4.92\\n25.8\\n65\\n(A)\\n1\\n512\\n512\\n5.29\\n24.9\\n4\\n128\\n128\\n5.00\\n25.5\\n16\\n32\\n32\\n4.91\\n25.8\\n32\\n16\\n16\\n5.01\\n25.4\\n(B)\\n16\\n5.16\\n25.1\\n58\\n32\\n5.01\\n25.4\\n60\\n(C)\\n2\\n6.11\\n23.7\\n36\\n4\\n5.19\\n25.3\\n50\\n8\\n4.88\\n25.5\\n80\\n256\\n32\\n32\\n5.75\\n24.5\\n28\\n1024\\n128\\n128\\n4.66\\n26.0\\n168\\n1024\\n5.12\\n25.4\\n53\\n4096\\n4.75\\n26.2\\n90\\n(D)\\n0.0\\n5.77\\n24.6\\n0.2\\n4.95\\n25.5\\n0.0\\n4.67\\n25.3\\n0.2\\n5.47\\n25.7\\n(E)\\npositional embedding instead of sinusoids\\n4.92\\n25.7\\nbig\\n6\\n1024\\n4096\\n16\\n0.3\\n300K\\n4.33\\n26.4\\n213\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be bene\ufb01cial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-\ufb01tting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [8], and observe nearly identical\\nresults to the base model.\\n7\\nConclusion\\nIn this work, we presented the Transformer, the \ufb01rst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signi\ufb01cantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to ef\ufb01ciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements\\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9\\n\\n---\\n[Text from page 2, relevance: 0.21]\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht\u22121 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsigni\ufb01cant improvements in computational ef\ufb01ciency through factorization tricks [18] and conditional\\ncomputation [26], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for signi\ufb01cantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2\\nBackground\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more dif\ufb01cult to learn dependencies between distant positions [11]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 22, 23, 19].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [28].\\nTo the best of our knowledge, however, the Transformer is the \ufb01rst transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [14, 15] and [8].\\n3\\nModel Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[9], consuming the previously generated symbols as additional input when generating the next.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1\\nEncoder and Decoder Stacks\\nEncoder:\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The \ufb01rst is a multi-head self-attention mechanism, and the second is a simple, position-\\n2\\n\\n---\\n\\nGenerated Answer:\\nThe model architecture diagram shown in the paper consists of two main components: the encoder and the decoder. Both components are composed of stacked layers. The encoder includes a stack of N = 6 identical layers, each having two sub-layers - a multi-head self-attention mechanism and a simple, position-wise, fully connected layer. The decoder, on the other hand, also consists of a stack of N = 6 identical layers, but with an additional self-attention mechanism. The encoder and decoder stacks are then connected by a residual connection around each of the sub-layers in the model. All sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512.'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_with_rag(\"Describe the model architecture diagram shown in the paper. What are its main components?\", chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 558
    },
    "id": "vJ6A75O9Riac",
    "outputId": "eb08e1b8-2677-4b41-ae9c-ef8613a6bd8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-17 22:53:03,675 - INFO - \n",
      "================================================================================\n",
      "2025-10-17 22:53:03,676 - INFO - Processing query: How does the Transformer model compare to RNN and CNN architectures according to the paper?\n",
      "2025-10-17 22:53:03,677 - INFO - ================================================================================\n",
      "\n",
      "2025-10-17 22:53:03,678 - INFO - Step 1: Retrieving relevant context...\n",
      "2025-10-17 22:53:03,678 - INFO - Querying with text: 'How does the Transformer model compare to RNN and CNN architectures according to the paper?'\n",
      "2025-10-17 22:53:03,775 - INFO - Found 3 results\n",
      "2025-10-17 22:53:03,776 - INFO - \n",
      "Step 2: Formatting context...\n",
      "2025-10-17 22:53:03,776 - INFO - Context length: 7583 characters\n",
      "2025-10-17 22:53:03,777 - INFO - \n",
      "Step 3: Generating response with PHI-3...\n",
      "2025-10-17 22:53:03,784 - INFO - Generating response...\n",
      "2025-10-17 22:53:13,994 - INFO - \n",
      "================================================================================\n",
      "2025-10-17 22:53:13,995 - INFO - COMPLETE\n",
      "2025-10-17 22:53:13,995 - INFO - ================================================================================\n",
      "\n",
      "\n",
      "Generated Answer:\n",
      "According to the paper, the Transformer model outperforms architectures based on recurrent or convolutional layers, such as RNN and CNN, on translation tasks. It can be trained significantly faster than these architectures. The paper presents the Transformer as the first sequence transduction model that is entirely based on attention, replacing the recurrent layers commonly used in encoder-decoder architectures. The authors acknowledge that determining compatibility is not easy and that a more sophisticated compatibility function than the dot product may be beneficial. They also plan to apply the Transformer to other tasks, including input and output modalities other than text, and investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio, and video.\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Retrieved Context:\\n---\\n[Text from page 9, relevance: 0.57]\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN\\ndmodel\\ndff\\nh\\ndk\\ndv\\nPdrop\\n\u03f5ls\\ntrain\\nPPL\\nBLEU\\nparams\\nsteps\\n(dev)\\n(dev)\\n\u00d7106\\nbase\\n6\\n512\\n2048\\n8\\n64\\n64\\n0.1\\n0.1\\n100K\\n4.92\\n25.8\\n65\\n(A)\\n1\\n512\\n512\\n5.29\\n24.9\\n4\\n128\\n128\\n5.00\\n25.5\\n16\\n32\\n32\\n4.91\\n25.8\\n32\\n16\\n16\\n5.01\\n25.4\\n(B)\\n16\\n5.16\\n25.1\\n58\\n32\\n5.01\\n25.4\\n60\\n(C)\\n2\\n6.11\\n23.7\\n36\\n4\\n5.19\\n25.3\\n50\\n8\\n4.88\\n25.5\\n80\\n256\\n32\\n32\\n5.75\\n24.5\\n28\\n1024\\n128\\n128\\n4.66\\n26.0\\n168\\n1024\\n5.12\\n25.4\\n53\\n4096\\n4.75\\n26.2\\n90\\n(D)\\n0.0\\n5.77\\n24.6\\n0.2\\n4.95\\n25.5\\n0.0\\n4.67\\n25.3\\n0.2\\n5.47\\n25.7\\n(E)\\npositional embedding instead of sinusoids\\n4.92\\n25.7\\nbig\\n6\\n1024\\n4096\\n16\\n0.3\\n300K\\n4.33\\n26.4\\n213\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be bene\ufb01cial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-\ufb01tting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [8], and observe nearly identical\\nresults to the base model.\\n7\\nConclusion\\nIn this work, we presented the Transformer, the \ufb01rst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signi\ufb01cantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to ef\ufb01ciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements\\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9\\n\\n---\\n[Text from page 3, relevance: 0.57]\\nFigure 1: The Transformer - model architecture.\\nwise fully connected feed-forward network. We employ a residual connection [10] around each of\\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder:\\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2\\nAttention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1\\nScaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\n3\\n\\n---\\n[Text from page 10, relevance: 0.56]\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, \u00c7aglar G\u00fcl\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[9] Alex Graves.\\nGenerating sequences with recurrent neural networks.\\narXiv preprint\\narXiv:1308.0850, 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770\u2013778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\u00fcrgen Schmidhuber. Gradient \ufb02ow in\\nrecurrent nets: the dif\ufb01culty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735\u20131780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[14] \u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[20] Samy Bengio \u0141ukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n10\\n\\n---\\n\\nGenerated Answer:\\nAccording to the paper, the Transformer model outperforms architectures based on recurrent or convolutional layers, such as RNN and CNN, on translation tasks. It can be trained significantly faster than these architectures. The paper presents the Transformer as the first sequence transduction model that is entirely based on attention, replacing the recurrent layers commonly used in encoder-decoder architectures. The authors acknowledge that determining compatibility is not easy and that a more sophisticated compatibility function than the dot product may be beneficial. They also plan to apply the Transformer to other tasks, including input and output modalities other than text, and investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio, and video.'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_with_rag(\"How does the Transformer model compare to RNN and CNN architectures according to the paper?\", chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 784
    },
    "id": "z9e3I0sKRk6Q",
    "outputId": "108aeb86-61aa-489b-f39e-e4cc8a9d6c22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-17 22:53:14,001 - INFO - \n",
      "================================================================================\n",
      "2025-10-17 22:53:14,002 - INFO - Processing query: What is the formula for scaled dot-product attention and what does each component represent?\n",
      "2025-10-17 22:53:14,003 - INFO - ================================================================================\n",
      "\n",
      "2025-10-17 22:53:14,003 - INFO - Step 1: Retrieving relevant context...\n",
      "2025-10-17 22:53:14,004 - INFO - Querying with text: 'What is the formula for scaled dot-product attention and what does each component represent?'\n",
      "2025-10-17 22:53:14,096 - INFO - Found 3 results\n",
      "2025-10-17 22:53:14,097 - INFO - \n",
      "Step 2: Formatting context...\n",
      "2025-10-17 22:53:14,097 - INFO - Context length: 9319 characters\n",
      "2025-10-17 22:53:14,098 - INFO - \n",
      "Step 3: Generating response with PHI-3...\n",
      "2025-10-17 22:53:14,106 - INFO - Generating response...\n",
      "2025-10-17 22:53:27,267 - INFO - \n",
      "================================================================================\n",
      "2025-10-17 22:53:27,268 - INFO - COMPLETE\n",
      "2025-10-17 22:53:27,269 - INFO - ================================================================================\n",
      "\n",
      "\n",
      "Generated Answer:\n",
      "The formula for scaled dot-product attention is as follows:\n",
      "\n",
      "Attention(Q, K, V ) = softmax(QKT \u221adk )V\n",
      "\n",
      "Here's what each component represents:\n",
      "\n",
      "- Q: Query matrix, which is a matrix of shape (batch_size, seq_len1, d_model)\n",
      "- K: Key matrix, which is a matrix of shape (batch_size, seq_len2, d_model)\n",
      "- V: Value matrix, which is a matrix of shape (batch_size, seq_len2, d_model)\n",
      "- T: Transpose of matrix K\n",
      "- d_model: Dimensionality of the model\n",
      "- \u221adk: Scaling factor\n",
      "\n",
      "The formula calculates the attention weights by multiplying Q and K, then applying a softmax function to obtain the weights on the values. The scaling factor is used to stabilize the gradient during backpropagation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Retrieved Context:\\n---\\n[Text from page 4, relevance: 0.73]\\nScaled Dot-Product Attention\\nMulti-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nquery with all keys, divide each by \u221adk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V ) = softmax(QKT\\n\u221adk\\n)V\\n(1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof\\n1\\n\u221adk . Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-ef\ufb01cient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by\\n1\\n\u221adk .\\n3.2.2\\nMulti-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it bene\ufb01cial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\noutput values. These are concatenated and once again projected, resulting in the \ufb01nal values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q \u00b7 k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4\\n\\n---\\n[Text from page 5, relevance: 0.43]\\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )\\nWhere the projections are parameter matrices W Q\\ni\\n\u2208Rdmodel\u00d7dk, W K\\ni\\n\u2208Rdmodel\u00d7dk, W V\\ni\\n\u2208Rdmodel\u00d7dv\\nand W O \u2208Rhdv\u00d7dmodel.\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3\\nApplications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n\u2022 In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[31, 2, 8].\\n\u2022 The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n\u2022 Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation \ufb02ow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to \u2212\u221e) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3\\nPosition-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2\\n(2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4\\nEmbeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [24]. In the embedding layers, we multiply those weights by \u221admodel.\\n3.5\\nPositional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\n5\\n\\n---\\n[Text from page 6, relevance: 0.34]\\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type\\nComplexity per Layer\\nSequential\\nMaximum Path Length\\nOperations\\nSelf-Attention\\nO(n2 \u00b7 d)\\nO(1)\\nO(1)\\nRecurrent\\nO(n \u00b7 d2)\\nO(n)\\nO(n)\\nConvolutional\\nO(k \u00b7 n \u00b7 d2)\\nO(1)\\nO(logk(n))\\nSelf-Attention (restricted)\\nO(r \u00b7 n \u00b7 d)\\nO(1)\\nO(n/r)\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and \ufb01xed [8].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel)\\nPE(pos,2i+1) = cos(pos/100002i/dmodel)\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2\u03c0 to 10000 \u00b7 2\u03c0. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any \ufb01xed offset k, PEpos+k can be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [8] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4\\nWhy Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi \u2208Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [11]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\nlength n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[31] and byte-pair [25] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\n6\\n\\n---\\n\\nGenerated Answer:\\nThe formula for scaled dot-product attention is as follows:\\n\\nAttention(Q, K, V ) = softmax(QKT \u221adk )V\\n\\nHere\\'s what each component represents:\\n\\n- Q: Query matrix, which is a matrix of shape (batch_size, seq_len1, d_model)\\n- K: Key matrix, which is a matrix of shape (batch_size, seq_len2, d_model)\\n- V: Value matrix, which is a matrix of shape (batch_size, seq_len2, d_model)\\n- T: Transpose of matrix K\\n- d_model: Dimensionality of the model\\n- \u221adk: Scaling factor\\n\\nThe formula calculates the attention weights by multiplying Q and K, then applying a softmax function to obtain the weights on the values. The scaling factor is used to stabilize the gradient during backpropagation.'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_with_rag(\"What is the formula for scaled dot-product attention and what does each component represent?\", chat_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2BMuBtGSS1Yt"
   },
   "source": [
    "### **Examples without Chat History**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RLhRhBYqTut4"
   },
   "source": [
    "#### *What is the main architecture proposed in the 'Attention is All You Need' paper?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "vrgCq82mS7b4",
    "outputId": "112002fc-c026-458f-ef60-28f7a9299c1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-17 22:53:27,278 - INFO - \n",
      "================================================================================\n",
      "2025-10-17 22:53:27,279 - INFO - Processing query: What is the main architecture proposed in the 'Attention is All You Need' paper?\n",
      "2025-10-17 22:53:27,280 - INFO - ================================================================================\n",
      "\n",
      "2025-10-17 22:53:27,280 - INFO - Step 1: Retrieving relevant context...\n",
      "2025-10-17 22:53:27,281 - INFO - Querying with text: 'What is the main architecture proposed in the 'Attention is All You Need' paper?'\n",
      "2025-10-17 22:53:27,377 - INFO - Found 3 results\n",
      "2025-10-17 22:53:27,378 - INFO - \n",
      "Step 2: Formatting context...\n",
      "2025-10-17 22:53:27,378 - INFO - Context length: 8682 characters\n",
      "2025-10-17 22:53:27,379 - INFO - \n",
      "Step 3: Generating response with PHI-3...\n",
      "2025-10-17 22:53:27,386 - INFO - Generating response...\n",
      "2025-10-17 22:53:30,252 - INFO - \n",
      "================================================================================\n",
      "2025-10-17 22:53:30,253 - INFO - COMPLETE\n",
      "2025-10-17 22:53:30,254 - INFO - ================================================================================\n",
      "\n",
      "\n",
      "Generated Answer:\n",
      "The main architecture proposed in the 'Attention is All You Need' paper is the Transformer model, which is based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Retrieved Context:\\n---\\n[Text from page 1, relevance: 0.44]\\nAttention Is All You Need\\nAshish Vaswani\u2217\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer\u2217\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar\u2217\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit\u2217\\nGoogle Research\\nusz@google.com\\nLlion Jones\u2217\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez\u2217\u2020\\nUniversity of Toronto\\naidan@cs.toronto.edu\\n\u0141ukasz Kaiser\u2217\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin\u2217\u2021\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signi\ufb01cantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\n1\\nIntroduction\\nRecurrent neural networks, long short-term memory [12] and gated recurrent [7] neural networks\\nin particular, have been \ufb01rmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [29, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the \ufb01rst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nef\ufb01cient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n\u2020Work performed while at Google Brain.\\n\u2021Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\n\\n---\\n[Text from page 5, relevance: 0.33]\\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )\\nWhere the projections are parameter matrices W Q\\ni\\n\u2208Rdmodel\u00d7dk, W K\\ni\\n\u2208Rdmodel\u00d7dk, W V\\ni\\n\u2208Rdmodel\u00d7dv\\nand W O \u2208Rhdv\u00d7dmodel.\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3\\nApplications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n\u2022 In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[31, 2, 8].\\n\u2022 The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n\u2022 Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation \ufb02ow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to \u2212\u221e) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3\\nPosition-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2\\n(2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4\\nEmbeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [24]. In the embedding layers, we multiply those weights by \u221admodel.\\n3.5\\nPositional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\n5\\n\\n---\\n[Text from page 4, relevance: 0.30]\\nScaled Dot-Product Attention\\nMulti-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nquery with all keys, divide each by \u221adk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V ) = softmax(QKT\\n\u221adk\\n)V\\n(1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof\\n1\\n\u221adk . Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-ef\ufb01cient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by\\n1\\n\u221adk .\\n3.2.2\\nMulti-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it bene\ufb01cial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\noutput values. These are concatenated and once again projected, resulting in the \ufb01nal values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q \u00b7 k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4\\n\\n---\\n\\nGenerated Answer:\\nThe main architecture proposed in the \\'Attention is All You Need\\' paper is the Transformer model, which is based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_with_rag(\"What is the main architecture proposed in the 'Attention is All You Need' paper?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1oHLoEYFTpbT"
   },
   "source": [
    "#### *Explain the multi-head attention mechanism.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 593
    },
    "id": "Ly2fWOQlS_Gt",
    "outputId": "6550704b-c8c0-428e-8325-a65e5d33c6e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-17 22:53:30,261 - INFO - \n",
      "================================================================================\n",
      "2025-10-17 22:53:30,262 - INFO - Processing query: Explain the multi-head attention mechanism.\n",
      "2025-10-17 22:53:30,263 - INFO - ================================================================================\n",
      "\n",
      "2025-10-17 22:53:30,263 - INFO - Step 1: Retrieving relevant context...\n",
      "2025-10-17 22:53:30,264 - INFO - Querying with text: 'Explain the multi-head attention mechanism.'\n",
      "2025-10-17 22:53:30,330 - INFO - Found 3 results\n",
      "2025-10-17 22:53:30,331 - INFO - \n",
      "Step 2: Formatting context...\n",
      "2025-10-17 22:53:30,331 - INFO - Context length: 10028 characters\n",
      "2025-10-17 22:53:30,332 - INFO - \n",
      "Step 3: Generating response with PHI-3...\n",
      "2025-10-17 22:53:30,340 - INFO - Generating response...\n",
      "2025-10-17 22:53:43,302 - INFO - \n",
      "================================================================================\n",
      "2025-10-17 22:53:43,303 - INFO - COMPLETE\n",
      "2025-10-17 22:53:43,303 - INFO - ================================================================================\n",
      "\n",
      "\n",
      "Generated Answer:\n",
      "The multi-head attention mechanism is a technique used in Transformer models that allows the model to jointly attend to information from different representation subspaces at different positions. It involves linearly projecting the queries, keys, and values h times with different, learned linear projections to dk, dk, and dv dimensions, respectively. Each of these projected versions of queries, keys, and values are then used to perform the attention function in parallel, resulting in dv-dimensional output values. These are concatenated and once again projected, resulting in the final values. \n",
      "\n",
      "Multi-head attention allows the model to model dependencies without regard to their distance in the input or output sequences, which is a fundamental constraint of sequential computation. This is achieved by factoring computation along the symbol positions of the input and output sequences. The multi-head attention mechanism has been shown to significantly improve model performance in various tasks, including sequence-to-sequence models and other sequence modeling tasks.\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Retrieved Context:\\n---\\n[Text from page 5, relevance: 0.70]\\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )\\nWhere the projections are parameter matrices W Q\\ni\\n\u2208Rdmodel\u00d7dk, W K\\ni\\n\u2208Rdmodel\u00d7dk, W V\\ni\\n\u2208Rdmodel\u00d7dv\\nand W O \u2208Rhdv\u00d7dmodel.\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3\\nApplications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n\u2022 In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[31, 2, 8].\\n\u2022 The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n\u2022 Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation \ufb02ow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to \u2212\u221e) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3\\nPosition-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2\\n(2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4\\nEmbeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [24]. In the embedding layers, we multiply those weights by \u221admodel.\\n3.5\\nPositional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\n5\\n\\n---\\n[Text from page 4, relevance: 0.62]\\nScaled Dot-Product Attention\\nMulti-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nquery with all keys, divide each by \u221adk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V ) = softmax(QKT\\n\u221adk\\n)V\\n(1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof\\n1\\n\u221adk . Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-ef\ufb01cient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by\\n1\\n\u221adk .\\n3.2.2\\nMulti-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it bene\ufb01cial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\noutput values. These are concatenated and once again projected, resulting in the \ufb01nal values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q \u00b7 k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4\\n\\n---\\n[Text from page 2, relevance: 0.46]\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht\u22121 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsigni\ufb01cant improvements in computational ef\ufb01ciency through factorization tricks [18] and conditional\\ncomputation [26], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for signi\ufb01cantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2\\nBackground\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more dif\ufb01cult to learn dependencies between distant positions [11]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 22, 23, 19].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [28].\\nTo the best of our knowledge, however, the Transformer is the \ufb01rst transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [14, 15] and [8].\\n3\\nModel Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[9], consuming the previously generated symbols as additional input when generating the next.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1\\nEncoder and Decoder Stacks\\nEncoder:\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The \ufb01rst is a multi-head self-attention mechanism, and the second is a simple, position-\\n2\\n\\n---\\n\\nGenerated Answer:\\nThe multi-head attention mechanism is a technique used in Transformer models that allows the model to jointly attend to information from different representation subspaces at different positions. It involves linearly projecting the queries, keys, and values h times with different, learned linear projections to dk, dk, and dv dimensions, respectively. Each of these projected versions of queries, keys, and values are then used to perform the attention function in parallel, resulting in dv-dimensional output values. These are concatenated and once again projected, resulting in the final values. \\n\\nMulti-head attention allows the model to model dependencies without regard to their distance in the input or output sequences, which is a fundamental constraint of sequential computation. This is achieved by factoring computation along the symbol positions of the input and output sequences. The multi-head attention mechanism has been shown to significantly improve model performance in various tasks, including sequence-to-sequence models and other sequence modeling tasks.'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_with_rag(\"Explain the multi-head attention mechanism.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wFqSUczqT24y"
   },
   "source": [
    "#### *Describe the model architecture diagram shown in the paper. What are its main components?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 558
    },
    "id": "qlzjyDQwT30x",
    "outputId": "3bb361fb-e2f5-48b3-ceed-cfc620be053b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-17 22:53:43,309 - INFO - \n",
      "================================================================================\n",
      "2025-10-17 22:53:43,310 - INFO - Processing query: Describe the model architecture diagram shown in the paper. What are its main components?\n",
      "2025-10-17 22:53:43,311 - INFO - ================================================================================\n",
      "\n",
      "2025-10-17 22:53:43,311 - INFO - Step 1: Retrieving relevant context...\n",
      "2025-10-17 22:53:43,312 - INFO - Querying with text: 'Describe the model architecture diagram shown in the paper. What are its main components?'\n",
      "2025-10-17 22:53:43,376 - INFO - Found 3 results\n",
      "2025-10-17 22:53:43,377 - INFO - \n",
      "Step 2: Formatting context...\n",
      "2025-10-17 22:53:43,378 - INFO - Context length: 8746 characters\n",
      "2025-10-17 22:53:43,378 - INFO - \n",
      "Step 3: Generating response with PHI-3...\n",
      "2025-10-17 22:53:43,386 - INFO - Generating response...\n",
      "2025-10-17 22:53:54,255 - INFO - \n",
      "================================================================================\n",
      "2025-10-17 22:53:54,256 - INFO - COMPLETE\n",
      "2025-10-17 22:53:54,256 - INFO - ================================================================================\n",
      "\n",
      "\n",
      "Generated Answer:\n",
      "The model architecture diagram shown in the paper consists of an encoder and a decoder. The encoder is composed of a stack of N=6 identical layers. Each layer has two sub-layers, a multi-head self-attention mechanism and a simple, position-wise, fully connected layer. The decoder is also composed of a stack of N=6 identical layers. Each layer in the decoder has three sub-layers, a multi-head self-attention mechanism, a position-wise, fully connected layer, and a position-wise feed-forward network. The decoder also employs a residual connection around each of the sub-layers, followed by layer normalization. All sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512.\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Retrieved Context:\\n---\\n[Text from page 3, relevance: 0.37]\\nFigure 1: The Transformer - model architecture.\\nwise fully connected feed-forward network. We employ a residual connection [10] around each of\\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder:\\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2\\nAttention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1\\nScaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\n3\\n\\n---\\n[Text from page 9, relevance: 0.27]\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN\\ndmodel\\ndff\\nh\\ndk\\ndv\\nPdrop\\n\u03f5ls\\ntrain\\nPPL\\nBLEU\\nparams\\nsteps\\n(dev)\\n(dev)\\n\u00d7106\\nbase\\n6\\n512\\n2048\\n8\\n64\\n64\\n0.1\\n0.1\\n100K\\n4.92\\n25.8\\n65\\n(A)\\n1\\n512\\n512\\n5.29\\n24.9\\n4\\n128\\n128\\n5.00\\n25.5\\n16\\n32\\n32\\n4.91\\n25.8\\n32\\n16\\n16\\n5.01\\n25.4\\n(B)\\n16\\n5.16\\n25.1\\n58\\n32\\n5.01\\n25.4\\n60\\n(C)\\n2\\n6.11\\n23.7\\n36\\n4\\n5.19\\n25.3\\n50\\n8\\n4.88\\n25.5\\n80\\n256\\n32\\n32\\n5.75\\n24.5\\n28\\n1024\\n128\\n128\\n4.66\\n26.0\\n168\\n1024\\n5.12\\n25.4\\n53\\n4096\\n4.75\\n26.2\\n90\\n(D)\\n0.0\\n5.77\\n24.6\\n0.2\\n4.95\\n25.5\\n0.0\\n4.67\\n25.3\\n0.2\\n5.47\\n25.7\\n(E)\\npositional embedding instead of sinusoids\\n4.92\\n25.7\\nbig\\n6\\n1024\\n4096\\n16\\n0.3\\n300K\\n4.33\\n26.4\\n213\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be bene\ufb01cial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-\ufb01tting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [8], and observe nearly identical\\nresults to the base model.\\n7\\nConclusion\\nIn this work, we presented the Transformer, the \ufb01rst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signi\ufb01cantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to ef\ufb01ciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements\\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9\\n\\n---\\n[Text from page 2, relevance: 0.21]\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht\u22121 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsigni\ufb01cant improvements in computational ef\ufb01ciency through factorization tricks [18] and conditional\\ncomputation [26], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for signi\ufb01cantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2\\nBackground\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more dif\ufb01cult to learn dependencies between distant positions [11]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 22, 23, 19].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [28].\\nTo the best of our knowledge, however, the Transformer is the \ufb01rst transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [14, 15] and [8].\\n3\\nModel Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[9], consuming the previously generated symbols as additional input when generating the next.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1\\nEncoder and Decoder Stacks\\nEncoder:\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The \ufb01rst is a multi-head self-attention mechanism, and the second is a simple, position-\\n2\\n\\n---\\n\\nGenerated Answer:\\nThe model architecture diagram shown in the paper consists of an encoder and a decoder. The encoder is composed of a stack of N=6 identical layers. Each layer has two sub-layers, a multi-head self-attention mechanism and a simple, position-wise, fully connected layer. The decoder is also composed of a stack of N=6 identical layers. Each layer in the decoder has three sub-layers, a multi-head self-attention mechanism, a position-wise, fully connected layer, and a position-wise feed-forward network. The decoder also employs a residual connection around each of the sub-layers, followed by layer normalization. All sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512.'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_with_rag(\"Describe the model architecture diagram shown in the paper. What are its main components?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ewuOxkebT_tG"
   },
   "source": [
    "#### *How does the Transformer model compare to RNN and CNN architectures according to the paper?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 558
    },
    "id": "GHnE1IHzUBWJ",
    "outputId": "b0488821-ffc3-4ff9-9b7e-5ac8a2e347c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-17 22:53:54,263 - INFO - \n",
      "================================================================================\n",
      "2025-10-17 22:53:54,264 - INFO - Processing query: How does the Transformer model compare to RNN and CNN architectures according to the paper?\n",
      "2025-10-17 22:53:54,265 - INFO - ================================================================================\n",
      "\n",
      "2025-10-17 22:53:54,265 - INFO - Step 1: Retrieving relevant context...\n",
      "2025-10-17 22:53:54,266 - INFO - Querying with text: 'How does the Transformer model compare to RNN and CNN architectures according to the paper?'\n",
      "2025-10-17 22:53:54,331 - INFO - Found 3 results\n",
      "2025-10-17 22:53:54,332 - INFO - \n",
      "Step 2: Formatting context...\n",
      "2025-10-17 22:53:54,332 - INFO - Context length: 7583 characters\n",
      "2025-10-17 22:53:54,333 - INFO - \n",
      "Step 3: Generating response with PHI-3...\n",
      "2025-10-17 22:53:54,340 - INFO - Generating response...\n",
      "2025-10-17 22:54:01,937 - INFO - \n",
      "================================================================================\n",
      "2025-10-17 22:54:01,938 - INFO - COMPLETE\n",
      "2025-10-17 22:54:01,939 - INFO - ================================================================================\n",
      "\n",
      "\n",
      "Generated Answer:\n",
      "According to the paper, the Transformer model compares favorably to RNN and CNN architectures for translation tasks. The paper states that the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, the Transformer achieved a new state of the art. This suggests that the Transformer model is more efficient and effective in translating languages compared to traditional RNN and CNN architectures.\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Retrieved Context:\\n---\\n[Text from page 9, relevance: 0.57]\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN\\ndmodel\\ndff\\nh\\ndk\\ndv\\nPdrop\\n\u03f5ls\\ntrain\\nPPL\\nBLEU\\nparams\\nsteps\\n(dev)\\n(dev)\\n\u00d7106\\nbase\\n6\\n512\\n2048\\n8\\n64\\n64\\n0.1\\n0.1\\n100K\\n4.92\\n25.8\\n65\\n(A)\\n1\\n512\\n512\\n5.29\\n24.9\\n4\\n128\\n128\\n5.00\\n25.5\\n16\\n32\\n32\\n4.91\\n25.8\\n32\\n16\\n16\\n5.01\\n25.4\\n(B)\\n16\\n5.16\\n25.1\\n58\\n32\\n5.01\\n25.4\\n60\\n(C)\\n2\\n6.11\\n23.7\\n36\\n4\\n5.19\\n25.3\\n50\\n8\\n4.88\\n25.5\\n80\\n256\\n32\\n32\\n5.75\\n24.5\\n28\\n1024\\n128\\n128\\n4.66\\n26.0\\n168\\n1024\\n5.12\\n25.4\\n53\\n4096\\n4.75\\n26.2\\n90\\n(D)\\n0.0\\n5.77\\n24.6\\n0.2\\n4.95\\n25.5\\n0.0\\n4.67\\n25.3\\n0.2\\n5.47\\n25.7\\n(E)\\npositional embedding instead of sinusoids\\n4.92\\n25.7\\nbig\\n6\\n1024\\n4096\\n16\\n0.3\\n300K\\n4.33\\n26.4\\n213\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be bene\ufb01cial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-\ufb01tting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [8], and observe nearly identical\\nresults to the base model.\\n7\\nConclusion\\nIn this work, we presented the Transformer, the \ufb01rst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signi\ufb01cantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to ef\ufb01ciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements\\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9\\n\\n---\\n[Text from page 3, relevance: 0.57]\\nFigure 1: The Transformer - model architecture.\\nwise fully connected feed-forward network. We employ a residual connection [10] around each of\\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder:\\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2\\nAttention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1\\nScaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\n3\\n\\n---\\n[Text from page 10, relevance: 0.56]\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, \u00c7aglar G\u00fcl\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[9] Alex Graves.\\nGenerating sequences with recurrent neural networks.\\narXiv preprint\\narXiv:1308.0850, 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770\u2013778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\u00fcrgen Schmidhuber. Gradient \ufb02ow in\\nrecurrent nets: the dif\ufb01culty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735\u20131780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[14] \u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[20] Samy Bengio \u0141ukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n10\\n\\n---\\n\\nGenerated Answer:\\nAccording to the paper, the Transformer model compares favorably to RNN and CNN architectures for translation tasks. The paper states that the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, the Transformer achieved a new state of the art. This suggests that the Transformer model is more efficient and effective in translating languages compared to traditional RNN and CNN architectures.'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_with_rag(\"How does the Transformer model compare to RNN and CNN architectures according to the paper?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ozf4gCTxUWAh"
   },
   "source": [
    "#### *What is the formula for scaled dot-product attention and what does each component represent?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 767
    },
    "id": "oHdsiL71UWxj",
    "outputId": "0750bbbc-4f26-4015-e889-1372565b16f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-17 22:54:01,945 - INFO - \n",
      "================================================================================\n",
      "2025-10-17 22:54:01,945 - INFO - Processing query: What is the formula for scaled dot-product attention and what does each component represent?\n",
      "2025-10-17 22:54:01,946 - INFO - ================================================================================\n",
      "\n",
      "2025-10-17 22:54:01,947 - INFO - Step 1: Retrieving relevant context...\n",
      "2025-10-17 22:54:01,947 - INFO - Querying with text: 'What is the formula for scaled dot-product attention and what does each component represent?'\n",
      "2025-10-17 22:54:02,020 - INFO - Found 3 results\n",
      "2025-10-17 22:54:02,021 - INFO - \n",
      "Step 2: Formatting context...\n",
      "2025-10-17 22:54:02,022 - INFO - Context length: 9319 characters\n",
      "2025-10-17 22:54:02,022 - INFO - \n",
      "Step 3: Generating response with PHI-3...\n",
      "2025-10-17 22:54:02,030 - INFO - Generating response...\n",
      "2025-10-17 22:54:18,070 - INFO - \n",
      "================================================================================\n",
      "2025-10-17 22:54:18,071 - INFO - COMPLETE\n",
      "2025-10-17 22:54:18,071 - INFO - ================================================================================\n",
      "\n",
      "\n",
      "Generated Answer:\n",
      "The formula for scaled dot-product attention is as follows:\n",
      "\n",
      "Attention(Q, K, V ) = softmax(QKT\u221adk)V\n",
      "\n",
      "Here is what each component represents:\n",
      "\n",
      "- Q, K, and V represent the query, key, and value matrices, respectively. These are matrices that contain the input representations of the sequence.\n",
      "- T represents the transpose of the key matrix K.\n",
      "- \u221adk represents the scaling factor, which is the square root of the dimensionality of the keys. This is used to prevent the values in the dot product from becoming too large, which can lead to numerical instability.\n",
      "- softmax is a function that applies a temperature-scaled softmax function to the dot product of the query and key matrices, resulting in a probability distribution over the values.\n",
      "- V represents the output matrix, which contains the attention-weighted values.\n",
      "\n",
      "The scaled dot-product attention mechanism allows the model to attend to different positions in the input sequence by computing a weighted sum of the values, with the weights determined by the similarity between the query and key matrices. The scaling factor helps to ensure numerical stability and improve the efficiency of the computation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Retrieved Context:\\n---\\n[Text from page 4, relevance: 0.73]\\nScaled Dot-Product Attention\\nMulti-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nquery with all keys, divide each by \u221adk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V ) = softmax(QKT\\n\u221adk\\n)V\\n(1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof\\n1\\n\u221adk . Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-ef\ufb01cient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by\\n1\\n\u221adk .\\n3.2.2\\nMulti-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it bene\ufb01cial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\noutput values. These are concatenated and once again projected, resulting in the \ufb01nal values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q \u00b7 k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4\\n\\n---\\n[Text from page 5, relevance: 0.43]\\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )\\nWhere the projections are parameter matrices W Q\\ni\\n\u2208Rdmodel\u00d7dk, W K\\ni\\n\u2208Rdmodel\u00d7dk, W V\\ni\\n\u2208Rdmodel\u00d7dv\\nand W O \u2208Rhdv\u00d7dmodel.\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3\\nApplications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n\u2022 In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[31, 2, 8].\\n\u2022 The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n\u2022 Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation \ufb02ow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to \u2212\u221e) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3\\nPosition-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2\\n(2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4\\nEmbeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [24]. In the embedding layers, we multiply those weights by \u221admodel.\\n3.5\\nPositional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\n5\\n\\n---\\n[Text from page 6, relevance: 0.34]\\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type\\nComplexity per Layer\\nSequential\\nMaximum Path Length\\nOperations\\nSelf-Attention\\nO(n2 \u00b7 d)\\nO(1)\\nO(1)\\nRecurrent\\nO(n \u00b7 d2)\\nO(n)\\nO(n)\\nConvolutional\\nO(k \u00b7 n \u00b7 d2)\\nO(1)\\nO(logk(n))\\nSelf-Attention (restricted)\\nO(r \u00b7 n \u00b7 d)\\nO(1)\\nO(n/r)\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and \ufb01xed [8].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel)\\nPE(pos,2i+1) = cos(pos/100002i/dmodel)\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2\u03c0 to 10000 \u00b7 2\u03c0. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any \ufb01xed offset k, PEpos+k can be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [8] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4\\nWhy Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi \u2208Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [11]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\nlength n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[31] and byte-pair [25] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\n6\\n\\n---\\n\\nGenerated Answer:\\nThe formula for scaled dot-product attention is as follows:\\n\\nAttention(Q, K, V ) = softmax(QKT\u221adk)V\\n\\nHere is what each component represents:\\n\\n- Q, K, and V represent the query, key, and value matrices, respectively. These are matrices that contain the input representations of the sequence.\\n- T represents the transpose of the key matrix K.\\n- \u221adk represents the scaling factor, which is the square root of the dimensionality of the keys. This is used to prevent the values in the dot product from becoming too large, which can lead to numerical instability.\\n- softmax is a function that applies a temperature-scaled softmax function to the dot product of the query and key matrices, resulting in a probability distribution over the values.\\n- V represents the output matrix, which contains the attention-weighted values.\\n\\nThe scaled dot-product attention mechanism allows the model to attend to different positions in the input sequence by computing a weighted sum of the values, with the weights determined by the similarity between the query and key matrices. The scaling factor helps to ensure numerical stability and improve the efficiency of the computation.'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_with_rag(\"What is the formula for scaled dot-product attention and what does each component represent?\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}